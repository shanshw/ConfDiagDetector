hadoop.http.idle_timeout.ms:NN/JN/DN Server connection timeout in milliseconds.
hadoop.security.dns.log-slow-lookups.threshold.ms:If slow lookup logging is enabled, this threshold is used to decide if a lookup is considered slow enough to be logged.
hadoop.security.groups.cache.secs:This is the config controlling the validity of the entries in the cache
    containing the user->group mapping. When this duration has expired,
    then the implementation of the group mapping provider is invoked to get
    the groups of the user and then cached back.
hadoop.security.groups.negative-cache.secs:Expiration time for entries in the the negative user-to-group mapping
    caching, in seconds. This is useful when invalid users are retrying
    frequently. It is suggested to set a small value for this expiration, since
    a transient error in group lookup could temporarily lock out a legitimate
    user.

    Set this to zero or negative value to disable negative user-to-group caching.
hadoop.security.groups.cache.warn.after.ms:If looking up a single user to group takes longer than this amount of
    milliseconds, we will log a warning message.
hadoop.security.groups.cache.background.reload.threads:Only relevant if hadoop.security.groups.cache.background.reload is true.
    Controls the number of concurrent background user->group cache entry
    refreshes. Pending refresh requests beyond this value are queued and
    processed when a thread is free.
hadoop.security.group.mapping.ldap.connection.timeout.ms:This property is the connection timeout (in milliseconds) for LDAP
    operations. If the LDAP provider doesn't establish a connection within the
    specified period, it will abort the connect attempt. Non-positive value
    means no LDAP connection timeout is specified in which case it waits for the
    connection to establish until the underlying network times out.
hadoop.security.group.mapping.ldap.read.timeout.ms:This property is the read timeout (in milliseconds) for LDAP
    operations. If the LDAP provider doesn't get a LDAP response within the
    specified period, it will abort the read attempt. Non-positive value
    means no read timeout is specified in which case it waits for the response
    infinitely.
hadoop.security.group.mapping.ldap.num.attempts:This property is the number of attempts to be made for LDAP operations.
    If this limit is exceeded, LdapGroupsMapping will return an empty
    group list.
hadoop.security.group.mapping.ldap.num.attempts.before.failover:This property is the number of attempts to be made for LDAP operations
    using a single LDAP instance. If multiple LDAP servers are configured
    and this number of failed operations is reached, we will switch to the
    next LDAP server. The configuration for the overall number of attempts
    will still be respected, failover will thus be performed only if this
    property is less than hadoop.security.group.mapping.ldap.num.attempts.
hadoop.security.group.mapping.ldap.search.group.hierarchy.levels:The number of levels to go up the group hierarchy when determining
    which groups a user is part of. 0 Will represent checking just the
    group that the user belongs to.  Each additional level will raise the
    time it takes to execute a query by at most
    hadoop.security.group.mapping.ldap.directory.search.timeout.
    The default will usually be appropriate for all LDAP systems.
hadoop.security.group.mapping.ldap.directory.search.timeout:The attribute applied to the LDAP SearchControl properties to set a
    maximum time limit when searching and awaiting a result.
    Set to 0 if infinite wait period is desired.
    Default is 10 seconds. Units in milliseconds.
hadoop.security.uid.cache.secs:This is the config controlling the validity of the entries in the cache
        containing the userId to userName and groupId to groupName used by
        NativeIO getFstat().
hadoop.kerberos.min.seconds.before.relogin:The minimum time between relogin attempts for Kerberos, in
    seconds.
io.file.buffer.size:The size of buffer for use in sequence files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations.
io.bytes.per.checksum:The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size.
io.map.index.skip:Number of index entries to skip between each entry.
  Zero by default. Setting this to values larger than zero can
  facilitate opening large MapFiles using less memory.
io.map.index.interval:MapFile consist of two files - data file (tuples) and index file
    (keys). For every io.map.index.interval records written in the
    data file, an entry (record-key, data-file-position) is written
    in the index file. This is to allow for doing binary search later
    within the index file to look up records by their keys and get their
    closest positions in the data file.
fs.trash.interval:Number of minutes after which the checkpoint
  gets deleted.  If zero, the trash feature is disabled.
  This option may be configured both on the server and the
  client. If trash is disabled server side then the client
  side configuration is checked. If trash is enabled on the
  server side then the value configured on the server is
  used and the client configuration value is ignored.
fs.trash.checkpoint.interval:Number of minutes between trash checkpoints.
  Should be smaller or equal to fs.trash.interval. If zero,
  the value is set to the value of fs.trash.interval.
  Every time the checkpointer runs it creates a new checkpoint
  out of current and removes checkpoints created more than
  fs.trash.interval minutes ago.
fs.ftp.host.port:FTP filesystem connects to fs.ftp.host on this port
fs.ftp.timeout:FTP filesystem's timeout in seconds.
fs.df.interval:Disk usage statistics refresh interval in msec.
fs.du.interval:File space usage statistics refresh interval in msec.
fs.s3a.connection.maximum:Controls the maximum number of simultaneous connections to S3.
    This must be bigger than the value of fs.s3a.threads.max so as to stop
    threads being blocked waiting for new HTTPS connections.
    Why not equal? The AWS SDK transfer manager also uses these connections.
fs.s3a.attempts.maximum:How many times we should retry commands on transient errors.
fs.s3a.connection.establish.timeout:Socket connection setup timeout in milliseconds.
fs.s3a.connection.timeout:Socket connection timeout in milliseconds.
fs.s3a.socket.send.buffer:Socket send buffer hint to amazon connector. Represented in bytes.
fs.s3a.socket.recv.buffer:Socket receive buffer hint to amazon connector. Represented in bytes.
fs.s3a.paging.maximum:How many keys to request from S3 when doing
     directory listings at a time.
fs.s3a.threads.max:The total number of threads available in the filesystem for data
    uploads *or any other queued filesystem operation*.
fs.s3a.threads.keepalivetime:Number of seconds a thread can be idle before being
    terminated.
fs.s3a.max.total.tasks:The number of operations which can be queued for execution.
  This is in addition to the number of active threads in fs.s3a.threads.max.
fs.s3a.executor.capacity:The maximum number of submitted tasks which is a single
    operation (e.g. rename(), delete()) may submit simultaneously for
    execution -excluding the IO-heavy block uploads, whose capacity
    is set in "fs.s3a.fast.upload.active.blocks"

    All tasks are submitted to the shared thread pool whose size is
    set in "fs.s3a.threads.max"; the value of capacity should be less than that
    of the thread pool itself, as the goal is to stop a single operation
    from overloading that thread pool.
fs.s3a.multipart.purge.age:Minimum age in seconds of multipart uploads to purge
    on startup if "fs.s3a.multipart.purge" is true
fs.s3a.fast.upload.active.blocks:Maximum Number of blocks a single output stream can have
    active (uploading, or queued to the central FileSystem
    instance's pool of queued operations.

    This stops a single stream overloading the shared thread pool.
fs.s3a.retry.limit:Number of times to retry any repeatable S3 client request on failure,
    excluding throttling requests.
fs.s3a.retry.throttle.limit:Number of times to retry any throttled request.
fs.s3a.committer.threads:Number of threads in committers for parallel operations on files
    (upload, commit, abort, delete...)
fs.s3a.list.version:Select which version of the S3 SDK's List Objects API to use.  Currently
    support 2 (default) and 1 (older API).
fs.s3a.connection.request.timeout:Time out on HTTP requests to the AWS service; 0 means no timeout.
    Measured in seconds; the usual time suffixes are all supported

    Important: this is the maximum duration of any AWS service call,
    including upload and copy operations. If non-zero, it must be larger
    than the time to upload multi-megabyte blocks to S3 from the client,
    and to rename many-GB files. Use with care.

    Values that are larger than Integer.MAX_VALUE milliseconds are
    converged to Integer.MAX_VALUE milliseconds
io.seqfile.compress.blocksize:The minimum block size for compression in block compressed
          SequenceFiles.
io.mapfile.bloom.size:The size of BloomFilter-s used in BloomMapFile. Each time this many
  keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter).
  Larger values minimize the number of filters, which slightly increases the performance,
  but may waste too much space if the total number of keys is usually much smaller
  than this number.
ipc.client.idlethreshold:Defines the threshold number of connections after which
               connections will be inspected for idleness.
ipc.client.kill.max:Defines the maximum number of clients to disconnect in one go.
ipc.client.connection.maxidletime:The maximum time in msec after which a client will bring down the
               connection to the server.
ipc.client.connect.max.retries:Indicates the number of retries a client will make to establish
               a server connection.
ipc.client.connect.retry.interval:Indicates the number of milliseconds a client will wait for
    before retrying to establish a server connection.
ipc.client.connect.timeout:Indicates the number of milliseconds a client will wait for the
               socket to establish a server connection.
ipc.client.connect.max.retries.on.timeouts:Indicates the number of retries a client will make on socket timeout
               to establish a server connection.
ipc.ping.interval:Timeout on waiting response from server, in milliseconds.
  The client will send ping when the interval is passed without receiving bytes,
  if ipc.client.ping is set to true.
ipc.client.rpc-timeout.ms:Timeout on waiting response from server, in milliseconds.
  If ipc.client.ping is set to true and this rpc-timeout is greater than
  the value of ipc.ping.interval, the effective value of the rpc-timeout is
  rounded up to multiple of ipc.ping.interval.
ipc.server.listen.queue.size:Indicates the length of the listen queue for servers accepting
               client connections.
ipc.server.purge.interval:Define how often calls are cleaned up in the server.
    The default is 15 minutes. The unit is minutes.
ipc.maximum.data.length:This indicates the maximum IPC message length (bytes) that can be
    accepted by the server. Messages larger than this value are rejected by the
    immediately to avoid possible OOMs. This setting should rarely need to be
    changed.
ipc.maximum.response.length:This indicates the maximum IPC message length (bytes) that can be
    accepted by the client. Messages larger than this value are rejected
    immediately to avoid possible OOMs. This setting should rarely need to be
    changed.  Set to 0 to disable.
ipc.[port_number].scheduler.priority.levels:How many priority levels to use within the scheduler and call
    queue. This property applies to RpcScheduler and CallQueue.
ipc.[port_number].decay-scheduler.period-ms:How frequently the decay factor should be applied to the
    operation counts of users. Higher values have less overhead, but respond
    less quickly to changes in client behavior.
    This property applies to DecayRpcScheduler.
ipc.[port_number].decay-scheduler.metrics.top.user.count:The number of top (i.e., heaviest) users to emit metric
    information about. This property applies to DecayRpcScheduler.
ipc.[port_number].weighted-cost.lockshared:The weight multiplier to apply to the time spent in the
    processing phase which holds a shared (read) lock.
    This property applies to WeightedTimeCostProvider.
ipc.[port_number].weighted-cost.lockexclusive:The weight multiplier to apply to the time spent in the
    processing phase which holds an exclusive (write) lock.
    This property applies to WeightedTimeCostProvider.
ipc.[port_number].weighted-cost.handler:The weight multiplier to apply to the time spent in the
    HANDLER phase which do not involve holding a lock.
    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on
    this phase. This property applies to WeightedTimeCostProvider.
ipc.[port_number].weighted-cost.lockfree:The weight multiplier to apply to the time spent in the
    LOCKFREE phase which do not involve holding a lock.
    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on
    this phase. This property applies to WeightedTimeCostProvider.
ipc.[port_number].weighted-cost.response:The weight multiplier to apply to the time spent in the
    RESPONSE phase which do not involve holding a lock.
    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on
    this phase. This property applies to WeightedTimeCostProvider.
net.topology.script.number.args:The max number of args that the script configured with
    net.topology.script.file.name should be run with. Each arg is an
    IP address.
file.stream-buffer-size:The size of buffer to stream files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations.
file.bytes-per-checksum:The number of bytes per checksum.  Must not be larger than
  file.stream-buffer-size
file.client-write-packet-size:Packet size for clients to write
file.blocksize:Block size
file.replication:Replication factor
ftp.stream-buffer-size:The size of buffer to stream files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations.
ftp.bytes-per-checksum:The number of bytes per checksum.  Must not be larger than
  ftp.stream-buffer-size
ftp.client-write-packet-size:Packet size for clients to write
ftp.blocksize:Block size
ftp.replication:Replication factor
tfile.io.chunk.size:Value chunk size in bytes. Default  to
    1MB. Values of the length less than the chunk size is
    guaranteed to have known value length in read time (See also
    TFile.Reader.Scanner.Entry.isValueLengthKnown()).
tfile.fs.output.buffer.size:Buffer size used for FSDataOutputStream in bytes.
tfile.fs.input.buffer.size:Buffer size used for FSDataInputStream in bytes.
hadoop.http.authentication.token.validity:Indicates how long (in seconds) an authentication token is valid before it has
    to be renewed.
hadoop.http.cross-origin.max-age:The number of seconds a pre-flighted request can be cached
    for web services needing cross-origin (CORS) support.
dfs.ha.fencing.ssh.connect-timeout:SSH connection timeout, in milliseconds, to use with the builtin
    sshfence fencer.
ha.zookeeper.session-timeout.ms:The session timeout to use when the ZKFC connects to ZooKeeper.
    Setting this value to a lower value implies that server crashes
    will be detected more quickly, but risks triggering failover too
    aggressively in the case of a transient error or network blip.
fs.permissions.umask-mode:The umask used when creating files and directories.
    Can be in octal or in symbolic. Examples are:
    "022" (octal for u=rwx,g=r-x,o=r-x in symbolic),
    or "u=rwx,g=rwx,o=" (symbolic for 007 in octal).
ha.health-monitor.connect-retry-interval.ms:How often to retry connecting to the service.
ha.health-monitor.check-interval.ms:How often to check the service.
ha.health-monitor.sleep-after-disconnect.ms:How long to sleep after an unexpected RPC error.
ha.health-monitor.rpc.connect.max.retries:The number of retries on connect error when establishing RPC proxy
    connection to NameNode, used for monitorHealth() calls.
ha.health-monitor.rpc-timeout.ms:Timeout for the actual monitorHealth() calls.
ha.failover-controller.new-active.rpc-timeout.ms:Timeout that the FC waits for the new active to become active
ha.failover-controller.graceful-fence.rpc-timeout.ms:Timeout that the FC waits for the old active to go to standby
ha.failover-controller.graceful-fence.connection.retries:FC connection retries for graceful fencing
ha.failover-controller.active-standby-elector.zk.op.retries:The number of zookeeper operation retry times in ActiveStandbyElector
ha.failover-controller.cli-check.rpc-timeout.ms:Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState
hadoop.security.crypto.buffer.size:The buffer size used by CryptoInputStream and CryptoOutputStream.
hadoop.security.key.default.bitlength:The length (bits) of keys we want the KeyProvider to produce. Key length
    defines the upper-bound on an algorithm's security, ideally, it would
    coincide with the lower-bound on an algorithm's security.
hadoop.security.kms.client.authentication.retry-count:Number of time to retry connecting to KMS on authentication failure
hadoop.security.kms.client.encrypted.key.cache.size:Size of the EncryptedKeyVersion cache Queue for each key
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads:Number of threads to use for refilling depleted EncryptedKeyVersion
    cache Queues
hadoop.security.kms.client.encrypted.key.cache.expiry:Cache expiry time for a Key, after which the cache Queue for this
    key will be dropped. Default = 12hrs
hadoop.security.kms.client.timeout:Sets value for KMS client connection timeout, and the read timeout
    to KMS servers.
hadoop.security.kms.client.failover.sleep.base.millis:Expert only. The time to wait, in milliseconds, between failover
    attempts increases exponentially as a function of the number of
    attempts made so far, with a random factor of +/- 50%. This option
    specifies the base value used in the failover calculation. The
    first failover will retry immediately. The 2nd failover attempt
    will delay at least hadoop.security.client.failover.sleep.base.millis
    milliseconds. And so on.
hadoop.security.kms.client.failover.sleep.max.millis:Expert only. The time to wait, in milliseconds, between failover
    attempts increases exponentially as a function of the number of
    attempts made so far, with a random factor of +/- 50%. This option
    specifies the maximum value to wait between failovers.
    Specifically, the time between two failover attempts will not
    exceed +/- 50% of hadoop.security.client.failover.sleep.max.millis
    milliseconds.
ipc.server.max.connections:The maximum number of concurrent connections a server is allowed
    to accept. If this limit is exceeded, incoming connections will first fill
    the listen queue and then may go to an OS-specific listen overflow queue.
    The client may fail or timeout, but the server can avoid running out of file
    descriptors using this feature. 0 means no limit.
hadoop.registry.zk.session.timeout.ms:Zookeeper session timeout in milliseconds
hadoop.registry.zk.connection.timeout.ms:Zookeeper connection timeout in milliseconds
hadoop.registry.zk.retry.times:Zookeeper connection retry count before failing
hadoop.registry.zk.retry.interval.ms:
hadoop.registry.zk.retry.ceiling.ms:Zookeeper retry limit in milliseconds, during
      exponential backoff.

      This places a limit even
      if the retry times and interval limit, combined
      with the backoff policy, result in a long retry
      period
hadoop.shell.safely.delete.limit.num.files:Used by -safely option of hadoop fs shell -rm command to avoid
      accidental deletion of large directories. When enabled, the -rm command
      requires confirmation if the number of files to be deleted is greater than
      this limit.  The default limit is 100 files. The warning is disabled if
      the limit is 0 or the -safely is not specified in -rm command.
hadoop.caller.context.max.size:The maximum bytes a caller context string can have. If the
      passed caller context is longer than this maximum bytes, client will
      truncate it before sending to server. Note that the server may have a
      different maximum size, and will truncate the caller context to the
      maximum size it allows.
hadoop.caller.context.signature.max.size:The caller's signature (optional) is for offline validation. If the
      signature exceeds the maximum allowed bytes in server, the caller context
      will be abandoned, in which case the caller context will not be recorded
      in audit logs.
seq.io.sort.mb:The total amount of buffer memory to use while sorting files,
      while using SequenceFile.Sorter, in megabytes. By default,
      gives each merge stream 1MB, which should minimize seeks.
seq.io.sort.factor:The number of streams to merge at once while sorting
      files using SequenceFile.Sorter.
      This determines the number of open file handles.
hadoop.zk.num-retries:Number of tries to connect to ZooKeeper.
hadoop.zk.retry-interval-ms:Retry interval in milliseconds when connecting to ZooKeeper.
hadoop.zk.timeout-ms:ZooKeeper session timeout in milliseconds. Session expiration
    is managed by the ZooKeeper cluster itself, not by the client. This value is
    used by the cluster to determine when the client's session expires.
    Expirations happens when the cluster does not hear from the client within
    the specified session timeout period (i.e. no heartbeat).
fs.getspaceused.jitterMillis:fs space usage statistics refresh jitter in msec.
hadoop.hdfs.configuration.version:version of this configuration file
dfs.datanode.http.internal-proxy.port:The datanode's internal web proxy port.
    By default it selects a random port available in runtime.
dfs.datanode.handler.count:The number of server threads for the datanode.
dfs.namenode.heartbeat.recheck-interval:This time decides the interval to check for expired datanodes.
    With this value and dfs.heartbeat.interval, the interval of
    deciding the datanode is stale or not is also calculated.
    The unit of this configuration is millisecond.
dfs.client.cached.conn.retry:The number of times the HDFS client will pull a socket from the
   cache.  Once this number is exceeded, the client will try to create a new
   socket.
dfs.default.chunk.view.size:The number of bytes to view for a file on the browser.
dfs.datanode.du.reserved:Reserved space in bytes per volume. Always leave this much space free for non dfs use.
      Specific storage type based reservation is also supported. The property can be followed with
      corresponding storage types ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage.
      For example, reserved space for RAM_DISK storage can be configured using property
      'dfs.datanode.du.reserved.ram_disk'. If specific storage type reservation is not configured
      then dfs.datanode.du.reserved will be used. Support multiple size unit suffix(case insensitive),
      as described in dfs.blocksize.
      Note: In case of using tune2fs to set reserved-blocks-percentage, or other filesystem tools,
      then you can possibly run into out of disk errors because hadoop will not check those
      external tool configurations.
dfs.datanode.du.reserved.pct:Reserved space in percentage. Read dfs.datanode.du.reserved.calculator to see
    when this takes effect. The actual number of bytes reserved will be calculated by using the
    total capacity of the data directory in question. Specific storage type based reservation
    is also supported. The property can be followed with corresponding storage types
    ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage.
    For example, reserved percentage space for RAM_DISK storage can be configured using property
    'dfs.datanode.du.reserved.pct.ram_disk'. If specific storage type reservation is not configured
    then dfs.datanode.du.reserved.pct will be used.
dfs.namenode.fs-limits.max-component-length:Defines the maximum number of bytes in UTF-8 encoding in each
      component of a path.  A value of 0 will disable the check. Support
      multiple size unit suffix(case insensitive), as described in dfs.blocksize.
dfs.namenode.fs-limits.max-directory-items:Defines the maximum number of items that a directory may
      contain. Cannot set the property to a value less than 1 or more than
      6400000.
dfs.namenode.fs-limits.min-block-size:Minimum block size in bytes, enforced by the Namenode at create
      time. This prevents the accidental creation of files with tiny block
      sizes (and thus many blocks), which can degrade performance. Support multiple
      size unit suffix(case insensitive), as described in dfs.blocksize.
dfs.namenode.fs-limits.max-blocks-per-file:Maximum number of blocks per file, enforced by the Namenode on
        write. This prevents the creation of extremely large files which can
        degrade performance.
dfs.namenode.lazypersist.file.scrub.interval.sec:The NameNode periodically scans the namespace for LazyPersist files with
    missing blocks and unlinks them from the namespace. This configuration key
    controls the interval between successive scans. If this value is set to 0,
    the file scrubber is disabled.
dfs.block.access.key.update.interval:Interval in minutes at which namenode updates its access keys.
dfs.block.access.token.lifetime:The lifetime of access tokens in minutes.
dfs.datanode.data.dir.perm:Permissions for the directories on on the local filesystem where
  the DFS data node store its blocks. The permissions can either be octal or
  symbolic.
dfs.replication:Default block replication. 
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
dfs.replication.max:Maximal block replication.
dfs.namenode.replication.min:Minimal block replication.
dfs.namenode.maintenance.replication.min:Minimal live block replication in existence of maintenance mode.
dfs.namenode.max-corrupt-file-blocks-returned:The maximum number of corrupt file blocks listed by NameNode Web UI,
      JMX and other client request.
dfs.blocksize:The default block size for new files, in bytes.
      You can use the following suffix (case insensitive):
      k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),
      Or provide complete size in bytes (such as 134217728 for 128 MB).
dfs.client.block.write.retries:The number of retries for writing blocks to the data nodes, 
  before we signal failure to the application.
dfs.client.block.write.replace-datanode-on-failure.min-replication:The minimum number of replications that are needed to not to fail
      the write pipeline if new datanodes can not be found to replace
      failed datanodes (could be due to network failure) in the write pipeline.
      If the number of the remaining datanodes in the write pipeline is greater
      than or equal to this property value, continue writing to the remaining nodes.
      Otherwise throw exception.

      If this is set to 0, an exception will be thrown, when a replacement
      can not be found.
      See also dfs.client.block.write.replace-datanode-on-failure.policy
dfs.blockreport.intervalMsec:Determines block reporting interval in milliseconds.
dfs.blockreport.initialDelay:Delay for first block report in seconds. Support multiple time unit
    suffix(case insensitive), as described in dfs.heartbeat.interval.If
    no time unit is specified then seconds is assumed
dfs.blockreport.split.threshold:If the number of blocks on the DataNode is below this
    threshold then it will send block reports for all Storage Directories
    in a single message.

    If the number of blocks exceeds this threshold then the DataNode will
    send block reports for each Storage Directory in separate messages.

    Set to zero to always split.
dfs.namenode.max.full.block.report.leases:The maximum number of leases for full block reports that the
    NameNode will issue at any given time.  This prevents the NameNode from
    being flooded with full block reports that use up all the RPC handler
    threads.  This number should never be more than the number of RPC handler
    threads or less than 1.
dfs.namenode.full.block.report.lease.length.ms:The number of milliseconds that the NameNode will wait before invalidating
    a full block report lease.  This prevents a crashed DataNode from
    permanently using up a full block report lease.
dfs.datanode.directoryscan.interval:Interval in seconds for Datanode to scan data directories and
  reconcile the difference between blocks in memory and on the disk.
  Support multiple time unit suffix(case insensitive), as described
  in dfs.heartbeat.interval.If no time unit is specified then seconds
  is assumed.
dfs.datanode.directoryscan.threads:How many threads should the threadpool used to compile reports
  for volumes in parallel have.
dfs.datanode.directoryscan.throttle.limit.ms.per.sec:The report compilation threads are limited to only running for
  a given number of milliseconds per second, as configured by the
  property. The limit is taken per thread, not in aggregate, e.g. setting
  a limit of 100ms for 4 compiler threads will result in each thread being
  limited to 100ms, not 25ms.

  Note that the throttle does not interrupt the report compiler threads, so the
  actual running time of the threads per second will typically be somewhat
  higher than the throttle limit, usually by no more than 20%.

  Setting this limit to 1000 disables compiler thread throttling. Only
  values between 1 and 1000 are valid. Setting an invalid value will result
  in the throttle being disabled and an error message being logged. 1000 is
  the default setting.
dfs.heartbeat.interval:Determines datanode heartbeat interval in seconds.
    Can use the following suffix (case insensitive):
    ms(millis), s(sec), m(min), h(hour), d(day)
    to specify the time (such as 2s, 2m, 1h, etc.).
    Or provide complete number in seconds (such as 30 for 30 seconds).
    If no time unit is specified then seconds is assumed.
dfs.namenode.handler.count:The number of Namenode RPC server threads that listen to
  requests from clients.
  If dfs.namenode.servicerpc-address is not configured then
  Namenode RPC server threads listen to requests from all nodes.
dfs.namenode.service.handler.count:The number of Namenode RPC server threads that listen to
  requests from DataNodes and from all other non-client nodes.
  dfs.namenode.service.handler.count will be valid only if
  dfs.namenode.servicerpc-address is configured.
dfs.namenode.safemode.min.datanodes:Specifies the number of datanodes that must be considered alive
    before the name node exits safemode.
    Values less than or equal to 0 mean not to take the number of live
    datanodes into account when deciding whether to remain in safe mode
    during startup.
    Values greater than the number of datanodes in the cluster
    will make safe mode permanent.
dfs.namenode.safemode.extension:Determines extension of safe mode in milliseconds after the threshold level
    is reached.  Support multiple time unit suffix (case insensitive), as
    described in dfs.heartbeat.interval.
dfs.namenode.resource.check.interval:The interval in milliseconds at which the NameNode resource checker runs.
    The checker calculates the number of the NameNode storage volumes whose
    available spaces are more than dfs.namenode.resource.du.reserved, and
    enters safemode if the number becomes lower than the minimum value
    specified by dfs.namenode.resource.checked.volumes.minimum.
dfs.namenode.resource.du.reserved:The amount of space to reserve/require for a NameNode storage directory
    in bytes. The default is 100MB. Support multiple size unit
    suffix(case insensitive), as described in dfs.blocksize.
dfs.namenode.resource.checked.volumes.minimum:The minimum number of redundant NameNode storage volumes required.
dfs.namenode.max.objects:The maximum number of files, directories and blocks
  dfs supports. A value of zero indicates no limit to the number
  of objects that dfs supports.
dfs.namenode.decommission.interval:Namenode periodicity in seconds to check if
    decommission or maintenance is complete. Support multiple time unit
    suffix(case insensitive), as described in dfs.heartbeat.interval.
    If no time unit is specified then seconds is assumed.
dfs.namenode.decommission.blocks.per.interval:The approximate number of blocks to process per decommission
    or maintenance interval, as defined in dfs.namenode.decommission.interval.
dfs.namenode.decommission.max.concurrent.tracked.nodes:The maximum number of decommission-in-progress or
    entering-maintenance datanodes nodes that will be tracked at one time by
    the namenode. Tracking these datanode consumes additional NN memory
    proportional to the number of blocks on the datnode. Having a conservative
    limit reduces the potential impact of decommissioning or maintenance of
    a large number of nodes at once.
      
    A value of 0 means no limit will be enforced.
dfs.namenode.decommission.backoff.monitor.pending.limit:When the Backoff monitor is enabled, determines the maximum number of blocks
    related to decommission and maintenance operations that can be loaded
    into the replication queue at any given time. Every
    dfs.namenode.decommission.interval seconds, the list is checked to see if
    the blocks have become fully replicated and then further blocks are added
    to reach the limit defined in this parameter.
dfs.namenode.decommission.backoff.monitor.pending.blocks.per.lock:When loading blocks into the replication queue, release the namenode write
    lock after the defined number of blocks have been processed.
dfs.namenode.redundancy.interval.seconds:The periodicity in seconds with which the namenode computes 
  low redundancy work for datanodes. Support multiple time unit suffix(case insensitive),
  as described in dfs.heartbeat.interval.
dfs.namenode.redundancy.queue.restart.iterations:When picking blocks from the low redundancy queues, reset the
    bookmarked iterator after the set number of iterations to ensure any blocks
    which were not processed on the first pass are retried before the iterators
    would naturally reach their end point. This ensures blocks are retried
    more frequently when there are many pending blocks or blocks are
    continuously added to the queues preventing the iterator reaching its
    natural endpoint.
    The default setting of 2400 combined with the default of
    dfs.namenode.redundancy.interval.seconds means the iterators will be reset
    approximately every 2 hours.
    Setting this parameter to zero disables the feature and the iterators will
    be reset only when the end of all queues has been reached.
dfs.namenode.accesstime.precision:The access time for HDFS file is precise upto this value. 
               The default value is 1 hour. Setting a value of 0 disables
               access times for HDFS.
dfs.stream-buffer-size:The size of buffer to stream files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations.
dfs.bytes-per-checksum:The number of bytes per checksum.  Must not be larger than
  dfs.stream-buffer-size
dfs.client-write-packet-size:Packet size for clients to write
dfs.client.write.exclude.nodes.cache.expiry.interval.millis:The maximum period to keep a DN in the excluded nodes list
  at a client. After this period, in milliseconds, the previously excluded node(s) will
  be removed automatically from the cache and will be considered good for block allocations
  again. Useful to lower or raise in situations where you keep a file open for very long
  periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance
  restarts. Defaults to 10 minutes.
dfs.namenode.checkpoint.period:The number of seconds between two periodic checkpoints.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.If no time unit is specified then seconds
    is assumed.
dfs.namenode.checkpoint.txns:The Secondary NameNode or CheckpointNode will create a checkpoint
  of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless
  of whether 'dfs.namenode.checkpoint.period' has expired.
dfs.namenode.checkpoint.check.period:The SecondaryNameNode and CheckpointNode will poll the NameNode
  every 'dfs.namenode.checkpoint.check.period' seconds to query the number
  of uncheckpointed transactions. Support multiple time unit suffix(case insensitive),
  as described in dfs.heartbeat.interval.If no time unit is specified then
  seconds is assumed.
dfs.namenode.checkpoint.max-retries:The SecondaryNameNode retries failed checkpointing. If the 
  failure occurs while loading fsimage or replaying edits, the number of
  retries is limited by this variable.
dfs.namenode.num.checkpoints.retained:The number of image checkpoint files (fsimage_*) that will be retained by
  the NameNode and Secondary NameNode in their storage directories. All edit
  logs (stored on edits_* files) necessary to recover an up-to-date namespace from the oldest retained
  checkpoint will also be retained.
dfs.namenode.num.extra.edits.retained:The number of extra transactions which should be retained
  beyond what is minimally necessary for a NN restart.
  It does not translate directly to file's age, or the number of files kept,
  but to the number of transactions (here "edits" means transactions).
  One edit file may contain several transactions (edits).
  During checkpoint, NameNode will identify the total number of edits to retain as extra by
  checking the latest checkpoint transaction value, subtracted by the value of this property.
  Then, it scans edits files to identify the older ones that don't include the computed range of
  retained transactions that are to be kept around, and purges them subsequently.
  The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have
  been offline for some time and need to have a longer backlog of retained
  edits in order to start again.
  Typically each edit is on the order of a few hundred bytes, so the default
  of 1 million edits should be on the order of hundreds of MBs or low GBs.

  NOTE: Fewer extra edits may be retained than value specified for this setting
  if doing so would mean that more segments would be retained than the number
  configured by dfs.namenode.max.extra.edits.segments.retained.
dfs.namenode.max.extra.edits.segments.retained:The maximum number of extra edit log segments which should be retained
  beyond what is minimally necessary for a NN restart. When used in conjunction with
  dfs.namenode.num.extra.edits.retained, this configuration property serves to cap
  the number of extra edits files to a reasonable value.
dfs.namenode.delegation.key.update-interval:The update interval for master key for delegation tokens 
       in the namenode in milliseconds.
dfs.namenode.delegation.token.max-lifetime:The maximum lifetime in milliseconds for which a delegation 
      token is valid.
dfs.namenode.delegation.token.renew-interval:The renewal interval for delegation token in milliseconds.
dfs.datanode.failed.volumes.tolerated:The number of volumes that are allowed to
  fail before a datanode stops offering service. By default
  any volume failure will cause a datanode to shutdown.
  The value should be greater than or equal to -1 , -1 represents minimum
  1 valid volume.
dfs.image.transfer.timeout:Socket timeout for the HttpURLConnection instance used in the image
        transfer. This is measured in milliseconds.
        This timeout prevents client hangs if the connection is idle
        for this configured timeout, during image transfer.
dfs.image.transfer.bandwidthPerSec:Maximum bandwidth used for regular image transfers (instead of
        bootstrapping the standby namenode), in bytes per second.
        This can help keep normal namenode operations responsive during
        checkpointing.
        A default value is 50mb per second.
        The maximum bandwidth used for bootstrapping standby namenode is
        configured with dfs.image.transfer-bootstrap-standby.bandwidthPerSec.
        Support multiple size unit suffix(case insensitive), as described
        in dfs.blocksize.
dfs.image.transfer-bootstrap-standby.bandwidthPerSec:Maximum bandwidth used for transferring image to bootstrap standby
      namenode, in bytes per second.
      A default value of 0 indicates that throttling is disabled. This default
      value should be used in most cases, to ensure timely HA operations.
      The maximum bandwidth used for regular image transfers is configured
      with dfs.image.transfer.bandwidthPerSec.
      Support multiple size unit suffix(case insensitive), as described in
      dfs.blocksize.
dfs.image.transfer.chunksize:Chunksize in bytes to upload the checkpoint.
        Chunked streaming is used to avoid internal buffering of contents
        of image file of huge size.
        Support multiple size unit suffix(case insensitive), as described
        in dfs.blocksize.
dfs.image.parallel.target.sections:Controls the number of sub-sections that will be written to
        fsimage for each section. This should be larger than
        dfs.image.parallel.threads, otherwise all threads will not be
        used when loading. Ideally, have at least twice the number
        of target sections as threads, so each thread must load more
        than one section to avoid one long running section affecting
        the load time.
dfs.image.parallel.inode.threshold:If the image contains less inodes than this setting, then
        do not write sub-sections and hence disable parallel loading.
        This is because small images load very quickly in serial and
        parallel loading is not needed.
dfs.image.parallel.threads:The number of threads to use when dfs.image.parallel.load is
        enabled. This setting should be less than
        dfs.image.parallel.target.sections. The optimal number of
        threads will depend on the hardware and environment.
dfs.edit.log.transfer.timeout:Socket timeout for edit log transfer in milliseconds. This timeout
    should be configured such that normal edit log transfer for journal
    node syncing can complete successfully.
dfs.edit.log.transfer.bandwidthPerSec:Maximum bandwidth used for transferring edit log to between journal nodes
    for syncing, in bytes per second.
    A default value of 0 indicates that throttling is disabled.
dfs.datanode.max.transfer.threads:Specifies the maximum number of threads to use for transferring data
        in and out of the DN.
dfs.datanode.scan.period.hours:If this is positive, the DataNode will not scan any
        individual block more than once in the specified scan period.
        If this is negative, the block scanner is disabled.
        If this is set to zero, then the default value of 504 hours
        or 3 weeks is used. Prior versions of HDFS incorrectly documented
        that setting this key to zero will disable the block scanner.
dfs.block.scanner.volume.bytes.per.second:If this is configured less than or equal to zero, the DataNode's block scanner will be disabled.  If this
        is positive, this is the number of bytes per second that the DataNode's
        block scanner will try to scan from each volume.
dfs.block.scanner.volume.join.timeout.ms:The amount of time in milliseconds that the BlockScanner times out waiting
    for the VolumeScanner thread to join during a shutdown call.
dfs.datanode.readahead.bytes:While reading block files, if the Hadoop native libraries are available,
        the datanode can use the posix_fadvise system call to explicitly
        page data into the operating system buffer cache ahead of the current
        reader's position. This can improve performance especially when
        disks are highly contended.

        This configuration specifies the number of bytes ahead of the current
        read position which the datanode will attempt to read ahead. This
        feature may be disabled by configuring this property to 0.

        If the native libraries are not available, this configuration has no
        effect.
dfs.client.failover.max.attempts:Expert only. The number of client failover attempts that should be
    made before the failover is considered failed.
dfs.client.failover.sleep.base.millis:Expert only. The time to wait, in milliseconds, between failover
    attempts increases exponentially as a function of the number of
    attempts made so far, with a random factor of +/- 50%. This option
    specifies the base value used in the failover calculation. The
    first failover will retry immediately. The 2nd failover attempt
    will delay at least dfs.client.failover.sleep.base.millis
    milliseconds. And so on.
dfs.client.failover.sleep.max.millis:Expert only. The time to wait, in milliseconds, between failover
    attempts increases exponentially as a function of the number of
    attempts made so far, with a random factor of +/- 50%. This option
    specifies the maximum value to wait between failovers. 
    Specifically, the time between two failover attempts will not
    exceed +/- 50% of dfs.client.failover.sleep.max.millis
    milliseconds.
dfs.client.failover.connection.retries:Expert only. Indicates the number of retries a failover IPC client
    will make to establish a server connection.
dfs.client.failover.connection.retries.on.timeouts:Expert only. The number of retry attempts a failover IPC client
    will make on socket timeout when establishing a server connection.
dfs.client.datanode-restart.timeout:Expert only. The time to wait, in seconds, from reception of an
    datanode shutdown notification for quick restart, until declaring
    the datanode dead and invoking the normal recovery mechanisms.
    The notification is sent by a datanode when it is being shutdown
    using the shutdownDatanode admin command with the upgrade option.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.If no time unit is specified then seconds
    is assumed.
dfs.ha.log-roll.period:How often, in seconds, the StandbyNode should ask the active to
    roll edit logs. Since the StandbyNode only reads from finalized
    log segments, the StandbyNode will only be as up-to-date as how
    often the logs are rolled. Note that failover triggers a log roll
    so the StandbyNode will be up to date before it becomes active.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.If no time unit is specified then seconds
    is assumed.
dfs.ha.tail-edits.period:How often, the StandbyNode and ObserverNode should check if there are new
    edit log entries ready to be consumed. This is the minimum period between
    checking; exponential backoff will be applied if no edits are found and
    dfs.ha.tail-edits.period.backoff-max is configured. By default, no
    backoff is applied.
    Supports multiple time unit suffix (case insensitive), as described
    in dfs.heartbeat.interval.
dfs.ha.tail-edits.period.backoff-max:The maximum time the tailer should wait between checking for new edit log
    entries. Exponential backoff will be applied when an edit log tail is
    performed but no edits are available to be read. Values less than or
    equal to zero disable backoff entirely; this is the default behavior.
    Supports multiple time unit suffix (case insensitive), as described
    in dfs.heartbeat.interval.
dfs.ha.tail-edits.namenode-retries:Number of retries to use when contacting the namenode when tailing the log.
dfs.ha.tail-edits.rolledits.timeout:The timeout in seconds of calling rollEdits RPC on Active NN.
dfs.short.circuit.shared.memory.watcher.interrupt.check.ms:The length of time in milliseconds that the short-circuit shared memory
    watcher will go between checking for java interruptions sent from other
    threads.  This is provided mainly for unit tests.
dfs.namenode.stale.datanode.interval:Default time interval in milliseconds for marking a datanode as "stale",
    i.e., if the namenode has not received heartbeat msg from a datanode for
    more than this time interval, the datanode will be marked and treated 
    as "stale" by default. The stale interval cannot be too small since 
    otherwise this may cause too frequent change of stale states. 
    We thus set a minimum stale interval value (the default value is 3 times 
    of heartbeat interval) and guarantee that the stale interval cannot be less
    than the minimum value. A stale data node is avoided during lease/block
    recovery. It can be conditionally avoided for reads (see
    dfs.namenode.avoid.read.stale.datanode) and for writes (see
    dfs.namenode.avoid.write.stale.datanode).
dfs.namenode.replication.work.multiplier.per.iteration:*Note*: Advanced property. Change with caution.
    This determines the total amount of block transfers to begin in
    parallel at a DN, for replication, when such a command list is being
    sent over a DN heartbeat by the NN. The actual number is obtained by
    multiplying this multiplier with the total number of live nodes in the
    cluster. The result number is the number of blocks to begin transfers
    immediately for, per DN heartbeat. This number can be any positive,
    non-zero integer.
nfs.server.port:Specify the port number used by Hadoop NFS.
nfs.mountd.port:Specify the port number used by Hadoop mount daemon.
nfs.rtmax:This is the maximum size in bytes of a READ request
    supported by the NFS gateway. If you change this, make sure you
    also update the nfs mount's rsize(add rsize= # of bytes to the 
    mount directive).
nfs.wtmax:This is the maximum size in bytes of a WRITE request
    supported by the NFS gateway. If you change this, make sure you
    also update the nfs mount's wsize(add wsize= # of bytes to the 
    mount directive).
hadoop.fuse.connection.timeout:The minimum number of seconds that we'll cache libhdfs connection objects
    in fuse_dfs. Lower values will result in lower memory consumption; higher
    values may speed up access by avoiding the overhead of creating new
    connection objects.
hadoop.fuse.timer.period:The number of seconds between cache expiry checks in fuse_dfs. Lower values
    will result in fuse_dfs noticing changes to Kerberos ticket caches more
    quickly.
dfs.namenode.metrics.logger.period.seconds:This setting controls how frequently the NameNode logs its metrics. The
    logging configuration must also define one or more appenders for
    NameNodeMetricsLog for the metrics to be logged.
    NameNode metrics logging is disabled if this value is set to zero or
    less than zero.
dfs.datanode.metrics.logger.period.seconds:This setting controls how frequently the DataNode logs its metrics. The
    logging configuration must also define one or more appenders for
    DataNodeMetricsLog for the metrics to be logged.
    DataNode metrics logging is disabled if this value is set to zero or
    less than zero.
dfs.datanode.peer.metrics.min.outlier.detection.samples:Minimum number of packet send samples which are required to qualify for outlier detection.
      If the number of samples is below this then outlier detection is skipped.
dfs.datanode.min.outlier.detection.nodes:Minimum number of nodes to run outlier detection.
dfs.datanode.slowpeer.low.threshold.ms:Threshold in milliseconds below which a DataNode is definitely not slow.
dfs.datanode.max.nodes.to.report:Number of nodes to include in JSON report. We will return nodes with
    the highest number of votes from peers.
dfs.namenode.max.slowpeer.collect.nodes:How many slow nodes we will collect for filtering out
    when choosing targets for blocks.

    It is ignored if dfs.namenode.block-placement-policy.exclude-slow-nodes.enabled is false.
dfs.datanode.fileio.profiling.sampling.percentage:This setting controls the percentage of file I/O events which will be
    profiled for DataNode disk statistics. The default value of 0 disables
    disk statistics. Set to an integer value between 1 and 100 to enable disk
    statistics.
dfs.datanode.min.outlier.detection.disks:Minimum number of disks to run outlier detection.
dfs.datanode.slowdisk.low.threshold.ms:Threshold in milliseconds below which a disk is definitely not slow.
dfs.datanode.max.disks.to.report:Number of disks to include in JSON report per operation. We will return
    disks with the highest latency.
dfs.datanode.max.slowdisks.to.exclude:The number of slow disks that needs to be excluded. By default, this parameter is set to 0,
    which disables excluding slow disk when choosing volume.
dfs.encrypt.data.transfer.cipher.key.bitlength:The key bitlength negotiated by dfsclient and datanode for encryption.
    This value may be set to either 128, 192 or 256.
dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold:Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to
    org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy.
    This setting controls how much DN volumes are allowed to differ in terms of
    bytes of free disk space before they are considered imbalanced. If the free
    space of all the volumes are within this range of each other, the volumes
    will be considered balanced and block assignments will be done on a pure
    round robin basis. Support multiple size unit suffix(case insensitive), as
    described in dfs.blocksize.
dfs.client.server-defaults.validity.period.ms:The amount of milliseconds after which cached server defaults are updated.

    By default this parameter is set to 1 hour.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.
dfs.namenode.retrycache.expirytime.millis:The time for which retry cache entries are retained.
dfs.client.mmap.cache.size:When zero-copy reads are used, the DFSClient keeps a cache of recently used
    memory mapped regions.  This parameter controls the maximum number of
    entries that we will keep in that cache.

    The larger this number is, the more file descriptors we will potentially
    use for memory-mapped files.  mmaped files also use virtual address space.
    You may need to increase your ulimit virtual address space limits before
    increasing the client mmap cache size.

    Note that you can still do zero-copy reads when this size is set to 0.
dfs.client.mmap.cache.timeout.ms:The minimum length of time that we will keep an mmap entry in the cache
    between uses.  If an entry is in the cache longer than this, and nobody
    uses it, it will be removed by a background thread.
dfs.client.mmap.retry.timeout.ms:The minimum amount of time that we will wait before retrying a failed mmap
    operation.
dfs.client.short.circuit.replica.stale.threshold.ms:The maximum amount of time that we will consider a short-circuit replica to
    be valid, if there is no communication from the DataNode.  After this time
    has elapsed, we will re-fetch the short-circuit replica even if it is in
    the cache.
dfs.datanode.max.locked.memory:The amount of memory in bytes to use for caching of block replicas in
    memory on the datanode. The datanode's maximum locked memory soft ulimit
    (RLIMIT_MEMLOCK) must be set to at least this value, else the datanode
    will abort on startup. Support multiple size unit suffix(case insensitive),
    as described in dfs.blocksize.

    By default, this parameter is set to 0, which disables in-memory caching.

    If the native libraries are not available to the DataNode, this
    configuration has no effect.
dfs.namenode.list.cache.directives.num.responses:This value controls the number of cache directives that the NameNode will
    send over the wire in response to a listDirectives RPC.
dfs.namenode.list.cache.pools.num.responses:This value controls the number of cache pools that the NameNode will
    send over the wire in response to a listPools RPC.
dfs.namenode.path.based.cache.refresh.interval.ms:The amount of milliseconds between subsequent path cache rescans.  Path
    cache rescans are when we calculate which blocks should be cached, and on
    what datanodes.

    By default, this parameter is set to 30 seconds.
dfs.namenode.path.based.cache.retry.interval.ms:When the NameNode needs to uncache something that is cached, or cache
    something that is not cached, it must direct the DataNodes to do so by
    sending a DNA_CACHE or DNA_UNCACHE command in response to a DataNode
    heartbeat.  This parameter controls how frequently the NameNode will
    resend these commands.
dfs.datanode.fsdatasetcache.max.threads.per.volume:The maximum number of threads per volume to use for caching new data
    on the datanode. These threads consume both I/O and CPU. This can affect
    normal datanode operations.
dfs.datanode.fsdatasetasyncdisk.max.threads.per.volume:The maximum number of threads per volume used to process async disk
    operations on the datanode. These threads consume I/O and CPU at the
    same time. This will affect normal data node operations.
dfs.cachereport.intervalMsec:Determines cache reporting interval in milliseconds.  After this amount of
    time, the DataNode sends a full report of its cache state to the NameNode.
    The NameNode uses the cache report to update its map of cached blocks to
    DataNode locations.

    This configuration has no effect if in-memory caching has been disabled by
    setting dfs.datanode.max.locked.memory to 0 (which is the default).

    If the native libraries are not available to the DataNode, this
    configuration has no effect.
dfs.namenode.edit.log.autoroll.check.interval.ms:How often an active namenode will check if it needs to roll its edit log,
    in milliseconds.
dfs.client.socket.send.buffer.size:Socket send buffer size for a write pipeline in DFSClient side.
    This may affect TCP connection throughput.
    If it is set to zero or negative value,
    no buffer size will be set explicitly,
    thus enable tcp auto-tuning on some system.
    The default value is 0.
dfs.domain.socket.disable.interval.seconds:The interval that a DataNode is disabled for future Short-Circuit Reads,
    after an error happens during a Short-Circuit Read. Setting this to 0 will
    not disable Short-Circuit Reads at all after errors happen. Negative values
    are invalid.
dfs.client.read.shortcircuit.streams.cache.size:The DFSClient maintains a cache of recently opened file descriptors.
    This parameter controls the maximum number of file descriptors in the cache.
    Setting this higher will use more file descriptors,
    but potentially provide better performance on workloads
    involving lots of seeks.
dfs.client.read.shortcircuit.streams.cache.expiry.ms:This controls the minimum amount of time
    file descriptors need to sit in the client cache context
    before they can be closed for being inactive for too long.
dfs.namenode.fs-limits.max-xattrs-per-inode:Maximum number of extended attributes per inode.
dfs.namenode.fs-limits.max-xattr-size:The maximum combined size of the name and value of an extended attribute
    in bytes. It should be larger than 0, and less than or equal to maximum
    size hard limit which is 32768.
    Support multiple size unit suffix(case insensitive), as described in
    dfs.blocksize.
dfs.client.slow.io.warning.threshold.ms:The threshold in milliseconds at which we will log a slow
    io warning in a dfsclient. By default, this parameter is set to 30000
    milliseconds (30 seconds).
dfs.datanode.slow.io.warning.threshold.ms:The threshold in milliseconds at which we will log a slow
    io warning in a datanode. By default, this parameter is set to 300
    milliseconds.
dfs.client.deadnode.detection.probe.deadnode.threads:The maximum number of threads to use for probing dead node.
dfs.client.deadnode.detection.idle.sleep.ms:The sleep time of DeadNodeDetector per iteration.
dfs.client.deadnode.detection.probe.suspectnode.threads:The maximum number of threads to use for probing suspect node.
dfs.client.deadnode.detection.rpc.threads:The maximum number of threads to use for calling RPC call to recheck the liveness of dead node.
dfs.client.deadnode.detection.probe.deadnode.interval.ms:Interval time in milliseconds for probing dead node behavior.
dfs.client.deadnode.detection.probe.suspectnode.interval.ms:Interval time in milliseconds for probing suspect node behavior.
dfs.client.deadnode.detection.probe.connection.timeout.ms:Connection timeout for probing dead node in milliseconds.
dfs.client.refresh.read-block-locations.ms:Refreshing LocatedBlocks period. A value of 0 disables the feature.
dfs.client.refresh.read-block-locations.threads:Number of threads to use for refreshing LocatedBlocks of registered
      DFSInputStreams. If a DFSClient opens many DFSInputStreams, increasing
      this may help refresh them all in a timely manner.
dfs.namenode.lease-recheck-interval-ms:During the release of lease a lock is hold that make any
    operations on the namenode stuck. In order to not block them during
    a too long duration we stop releasing lease after this max lock limit.
dfs.namenode.max-lock-hold-to-release-lease-ms:During the release of lease a lock is hold that make any
    operations on the namenode stuck. In order to not block them during
    a too long duration we stop releasing lease after this max lock limit.
dfs.namenode.write-lock-reporting-threshold-ms:When a write lock is held on the namenode for a long time,
    this will be logged as the lock is released. This sets how long the
    lock must be held for logging to occur.
dfs.namenode.read-lock-reporting-threshold-ms:When a read lock is held on the namenode for a long time,
    this will be logged as the lock is released. This sets how long the
    lock must be held for logging to occur.
dfs.datanode.lock-reporting-threshold-ms:When thread waits to obtain a lock, or a thread holds a lock for
    more than the threshold, a log message will be written. Note that
    dfs.lock.suppress.warning.interval ensures a single log message is
    emitted per interval for waiting threads and a single message for holding
    threads to avoid excessive logging.
dfs.namenode.startup.delay.block.deletion.sec:The delay in seconds at which we will pause the blocks deletion
    after Namenode startup. By default it's disabled.
    In the case a directory has large number of directories and files are
    deleted, suggested delay is one hour to give the administrator enough time
    to notice large number of pending deletion blocks and take corrective
    action.
dfs.datanode.block.id.layout.upgrade.threads:The number of threads to use when creating hard links from
    current to previous blocks during upgrade of a DataNode to block ID-based
    block layout (see HDFS-6482 for details on the layout).
dfs.namenode.list.encryption.zones.num.responses:When listing encryption zones, the maximum number of zones
    that will be returned in a batch. Fetching the list incrementally in
    batches improves namenode performance.
dfs.namenode.list.reencryption.status.num.responses:When listing re-encryption status, the maximum number of zones
    that will be returned in a batch. Fetching the list incrementally in
    batches improves namenode performance.
dfs.namenode.list.openfiles.num.responses:When listing open files, the maximum number of open files that will be
      returned in a single batch. Fetching the list incrementally in batches
      improves namenode performance.
dfs.namenode.edekcacheloader.interval.ms:When KeyProvider is configured, the interval time of warming
    up edek cache on NN starts up / becomes active. All edeks will be loaded
    from KMS into provider cache. The edek cache loader will try to warm up the
    cache until succeed or NN leaves active state.
dfs.namenode.edekcacheloader.initial.delay.ms:When KeyProvider is configured, the time delayed until the first
    attempt to warm up edek cache on NN start up / become active.
dfs.namenode.reencrypt.batch.size:How many EDEKs should the re-encrypt thread process in one batch.
dfs.namenode.reencrypt.edek.threads:Maximum number of re-encrypt threads to contact the KMS
    and re-encrypt the edeks.
dfs.namenode.inotify.max.events.per.rpc:Maximum number of events that will be sent to an inotify client
    in a single RPC response. The default value attempts to amortize away
    the overhead for this RPC while avoiding huge memory requirements for the
    client and NameNode (1000 events should consume no more than 1 MB.)
dfs.datanode.cache.revocation.timeout.ms:When the DFSClient reads from a block file which the DataNode is
    caching, the DFSClient can skip verifying checksums.  The DataNode will
    keep the block file in cache until the client is done.  If the client takes
    an unusually long time, though, the DataNode may need to evict the block
    file from the cache anyway.  This value controls how long the DataNode will
    wait for the client to release a replica that it is reading without
    checksums.
dfs.datanode.cache.revocation.polling.ms:How often the DataNode should poll to see if the clients have
    stopped using a replica that the DataNode wants to uncache.
dfs.namenode.top.window.num.buckets:Number of buckets in the rolling window implementation of nntop
dfs.namenode.top.num.users:Number of top users returned by the top tool
dfs.webhdfs.ugi.expire.after.access:How long in milliseconds after the last access
      the cached UGI will expire. With 0, never expire.
dfs.namenode.blocks.per.postponedblocks.rescan:Number of blocks to rescan for each iteration of
    postponedMisreplicatedBlocks.
dfs.client.block.write.locateFollowingBlock.initial.delay.ms:The initial delay (unit is ms) for locateFollowingBlock,
    the delay time will increase exponentially(double) for each retry
    until dfs.client.block.write.locateFollowingBlock.max.delay.ms is reached,
    after that the delay for each retry will be
    dfs.client.block.write.locateFollowingBlock.max.delay.ms.
dfs.client.block.write.locateFollowingBlock.max.delay.ms:The maximum delay (unit is ms) before retrying locateFollowingBlock.
dfs.ha.zkfc.nn.http.timeout.ms:The HTTP connection and read timeout value (unit is ms ) when DFS ZKFC
    tries to get local NN thread dump after local NN becomes
    SERVICE_NOT_RESPONDING or SERVICE_UNHEALTHY.
    If it is set to zero, DFS ZKFC won't get local NN thread dump.
dfs.namenode.ec.policies.max.cellsize:The maximum cell size of erasure coding policy. Default is 4MB.
dfs.datanode.ec.reconstruction.stripedread.timeout.millis:Datanode striped read timeout in milliseconds.
dfs.datanode.ec.reconstruction.stripedread.buffer.size:Datanode striped read buffer size.
dfs.datanode.ec.reconstruction.threads:Number of threads used by the Datanode for background
    reconstruction work.
dfs.namenode.quota.init-threads:The number of concurrent threads to be used in quota initialization. The
    speed of quota initialization also affects the namenode fail-over latency.
    If the size of name space is big, try increasing this to 16 or higher.
dfs.datanode.transfer.socket.send.buffer.size:Socket send buffer size for DataXceiver (mirroring packets to downstream
    in pipeline). This may affect TCP connection throughput.
    If it is set to zero or negative value, no buffer size will be set
    explicitly, thus enable tcp auto-tuning on some system.
    The default value is 0.
dfs.datanode.transfer.socket.recv.buffer.size:Socket receive buffer size for DataXceiver (receiving packets from client
    during block writing). This may affect TCP connection throughput.
    If it is set to zero or negative value, no buffer size will be set
    explicitly, thus enable tcp auto-tuning on some system.
    The default value is 0.
dfs.datanode.bp-ready.timeout:The maximum wait time for datanode to be ready before failing the
    received request. Setting this to 0 fails requests right away if the
    datanode is not yet registered with the namenode. This wait time
    reduces initial request failures after datanode restart.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.If no time unit is specified then seconds
    is assumed.
dfs.datanode.cached-dfsused.check.interval.ms:The interval check time of loading DU_CACHE_FILE in each volume.
    When the cluster doing the rolling upgrade operations, it will
    usually lead dfsUsed cache file of each volume expired and redo the
    du operations in datanode and that makes datanode start slowly. Adjust
    this property can make cache file be available for the time as you want.
dfs.http.client.failover.max.attempts:Specify the max number of failover attempts for WebHDFS client
    in case of network exception.
dfs.http.client.retry.max.attempts:Specify the max number of retry attempts for WebHDFS client,
    if the difference between retried attempts and failovered attempts is
    larger than the max number of retry attempts, there will be no more
    retries.
dfs.http.client.failover.sleep.base.millis:Specify the base amount of time in milliseconds upon which the
    exponentially increased sleep time between retries or failovers
    is calculated for WebHDFS client.
dfs.http.client.failover.sleep.max.millis:Specify the upper bound of sleep time in milliseconds between
    retries or failovers for WebHDFS client.
datanode.https.port:HTTPS port for DataNode.
dfs.namenode.get-blocks.max-qps:The maximum number of getBlocks RPCs data movement utilities can make to
    a NameNode per second. Values less than or equal to 0 disable throttling.
    This affects anything that uses a NameNodeConnector, i.e., the Balancer,
    Mover, and StoragePolicySatisfier.
dfs.balancer.dispatcherThreads:Size of the thread pool for the HDFS balancer block mover.
    dispatchExecutor
dfs.balancer.movedWinWidth:Window of time in ms for the HDFS balancer tracking blocks and its
    locations.
dfs.balancer.moverThreads:Thread pool size for executing block moves.
    moverThreadAllocator
dfs.balancer.max-size-to-move:Maximum number of bytes that can be moved by the balancer in a single
    thread.
dfs.balancer.getBlocks.min-block-size:Minimum block threshold size in bytes to ignore when fetching a source's
    block list.
dfs.balancer.getBlocks.size:Total size in bytes of Datanode blocks to get when fetching a source's
    block list.
dfs.balancer.block-move.timeout:Maximum amount of time in milliseconds for a block to move. If this is set
    greater than 0, Balancer will stop waiting for a block move completion
    after this time. In typical clusters, a 3 to 5 minute timeout is reasonable.
    If timeout happens to a large proportion of block moves, this needs to be
    increased. It could also be that too much work is dispatched and many nodes
    are constantly exceeding the bandwidth limit as a result. In that case,
    other balancer parameters might need to be adjusted.
    It is disabled (0) by default.
dfs.balancer.max-no-move-interval:If this specified amount of time has elapsed and no block has been moved
    out of a source DataNode, on more effort will be made to move blocks out of
    this DataNode in the current Balancer iteration.
dfs.balancer.max-iteration-time:Maximum amount of time while an iteration can be run by the Balancer. After
    this time the Balancer will stop the iteration, and reevaluate the work
    needs to be done to Balance the cluster. The default value is 20 minutes.
dfs.block.invalidate.limit:The maximum number of invalidate blocks sent by namenode to a datanode
    per heartbeat deletion command. This property works with
    "dfs.namenode.invalidate.work.pct.per.iteration" to throttle block
    deletions.
dfs.balancer.service.retries.on.exception:When the balancer is executed as a long-running service, it will retry upon encountering an exception. This
    configuration determines how many times it will retry before considering the exception to be fatal and quitting.
dfs.block.misreplication.processing.limit:Maximum number of blocks to process for initializing replication queues.
dfs.blockreport.incremental.intervalMsec:If set to a positive integer, the value in ms to wait between sending
    incremental block reports from the Datanode to the Namenode.
dfs.checksum.ec.socket-timeout:Default timeout value in milliseconds for computing the checksum of striped blocks.
    Recommended to set the same value between client and DNs in a cluster because mismatching
    may cause exhausting handler threads.
dfs.client.block.write.locateFollowingBlock.retries:Number of retries to use when finding the next block during HDFS writes.
dfs.client.key.provider.cache.expiry:DFS client security key cache expiration in milliseconds.
dfs.client.max.block.acquire.failures:Maximum failures allowed when trying to get block information from a specific datanode.
dfs.client.read.short.circuit.replica.stale.threshold.ms:Threshold in milliseconds for read entries during short-circuit local reads.
dfs.client.read.shortcircuit.buffer.size:Buffer size in bytes for short-circuit local reads.
dfs.client.short.circuit.num:Number of short-circuit caches. This setting should
    be in the range 1 - 5. Lower values will result in lower CPU consumption; higher
    values may speed up massive parallel reading files.
dfs.client.read.striped.threadpool.size:The maximum number of threads used for parallel reading
    in striped layout.
dfs.client.retry.interval-ms.get-last-block-length:Retry interval in milliseconds to wait between retries in getting
    block lengths from the datanodes.
dfs.client.retry.max.attempts:Max retry attempts for DFSClient talking to namenodes.
dfs.client.retry.times.get-last-block-length:Number of retries for calls to fetchLocatedBlocksAndGetLastBlockLength().
dfs.client.retry.window.base:Base time window in ms for DFSClient retries.  For each retry attempt,
    this value is extended linearly (e.g. 3000 ms for first attempt and
    first retry, 6000 ms for second retry, 9000 ms for third retry, etc.).
dfs.client.socket-timeout:Default timeout value in milliseconds for all sockets.
dfs.client.socketcache.capacity:Socket cache capacity (in entries) for short-circuit reads.
    If this value is set to 0, the client socket cache is disabled.
dfs.client.socketcache.expiryMsec:Socket cache expiration for short-circuit reads in msec.
dfs.client.test.drop.namenode.response.number:The number of Namenode responses dropped by DFSClient for each RPC call.  Used
    for testing the NN retry cache.
dfs.client.hedged.read.threadpool.size:Support 'hedged' reads in DFSClient. To enable this feature, set the parameter
    to a positive number. The threadpool size is how many threads to dedicate
    to the running of these 'hedged', concurrent reads in your client.
dfs.client.hedged.read.threshold.millis:Configure 'hedged' reads in DFSClient. This is the number of milliseconds
    to wait before starting up a 'hedged' read.
dfs.client.write.byte-array-manager.count-limit:The maximum number of arrays allowed for each array length.
dfs.client.write.byte-array-manager.count-reset-time-period-ms:The time period in milliseconds that the allocation count for each array length is
    reset to zero if there is no increment.
dfs.client.write.byte-array-manager.count-threshold:The count threshold for each array length so that a manager is created only after the
    allocation count exceeds the threshold. In other words, the particular array length
    is not managed until the allocation count exceeds the threshold.
dfs.client.write.max-packets-in-flight:The maximum number of DFSPackets allowed in flight.
dfs.client.block.reader.remote.buffer.size:The output stream buffer size of a DFSClient remote read. The buffer default value is 512B. The buffer includes
      only some request parameters that are: block, blockToken, clientName, startOffset, len, verifyChecksum,
      cachingStrategy.
dfs.content-summary.limit:The maximum content summary counts allowed in one locking period. 0 or a negative number
    means no limit (i.e. no yielding).
dfs.content-summary.sleep-microsec:The length of time in microseconds to put the thread to sleep, between reaquiring the locks
    in content summary computation.
dfs.datanode.balance.max.concurrent.moves:Maximum number of threads for Datanode balancer pending moves.  This
    value is reconfigurable via the "dfsadmin -reconfig" command.
dfs.datanode.data.transfer.bandwidthPerSec:Specifies the maximum amount of bandwidth that the data transfering can utilize for transfering block when
      BlockConstructionStage is
      PIPELINE_SETUP_CREATE and clientName is empty.
      When the bandwidth value is zero, there is no limit.
dfs.datanode.data.write.bandwidthPerSec:Specifies the maximum amount of bandwidth that the data transfering can utilize for writing block or pipeline
      recovery when
      BlockConstructionStage is PIPELINE_SETUP_APPEND_RECOVERY or PIPELINE_SETUP_STREAMING_RECOVERY.
      When the bandwidth value is zero, there is no limit.
dfs.datanode.ec.reconstruct.read.bandwidthPerSec:Specifies the maximum amount of bandwidth that the EC reconstruction can utilize for reading.
      When the bandwidth value is zero, there is no limit.
dfs.datanode.ec.reconstruct.write.bandwidthPerSec:Specifies the maximum amount of bandwidth that the EC reconstruction can utilize for writing.
      When the bandwidth value is zero, there is no limit.
dfs.datanode.lazywriter.interval.sec:Interval in seconds for Datanodes for lazy persist writes.
dfs.datanode.network.counts.cache.max.size:The maximum number of entries the datanode per-host network error
    count cache may contain.
dfs.datanode.restart.replica.expiration:During shutdown for restart, the amount of time in seconds budgeted for
    datanode restart.
dfs.datanode.socket.reuse.keepalive:The window of time in ms before the DataXceiver closes a socket for a
    single request.  If a second request occurs within that window, the
    socket can be reused.
dfs.datanode.socket.write.timeout:Timeout in ms for clients socket writes to DataNodes.
dfs.ha.zkfc.port:The port number that the zookeeper failover controller RPC
    server binds to.
dfs.journalnode.sync.interval:Time interval, in milliseconds, between two Journal Node syncs.
    This configuration takes effect only if the journalnode sync is enabled
    by setting the configuration parameter dfs.journalnode.enable.sync to true.
dfs.journalnode.edit-cache-size.bytes:The size, in bytes, of the in-memory cache of edits to keep on the
    JournalNode. This cache is used to serve edits for tailing via the RPC-based
    mechanism, and is only enabled when dfs.ha.tail-edits.in-progress is true.
    Transactions range in size but are around 200 bytes on average, so the
    default of 1MB can store around 5000 transactions.
dfs.batched.ls.limit:Limit the number of paths that can be listed in a single batched
    listing call. printed by ls. If less or equal to
    zero, at most DFS_LIST_LIMIT_DEFAULT (= 1000) will be printed.
dfs.ls.limit:Limit the number of files printed by ls. If less or equal to
    zero, at most DFS_LIST_LIMIT_DEFAULT (= 1000) will be printed.
dfs.mover.movedWinWidth:The minimum time interval, in milliseconds, that a block can be
    moved to another location again.
dfs.mover.moverThreads:Configure the balancer's mover thread pool size.
dfs.mover.retry.max.attempts:The maximum number of retries before the mover consider the
    move failed.
dfs.mover.max-no-move-interval:If this specified amount of time has elapsed and no block has been moved
    out of a source DataNode, on more effort will be made to move blocks out of
    this DataNode in the current Mover iteration.
dfs.namenode.audit.log.async.buffer.size:Only used when enables asynchronous audit log. Sets the number of audit
    logs allowed in the event buffer before the calling thread is blocked
    (if dfs.namenode.audit.log.async.blocking is true) or until logs are
    summarized and discarded. Default value is 128.
dfs.namenode.available-space-block-placement-policy.balanced-space-tolerance:Only used when the dfs.block.replicator.classname is set to
      org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.
      Special value between 0 and 20, inclusive. if the value is set beyond the scope,
      this value will be set as 5 by default, Increases tolerance of
      placing blocks on Datanodes with similar disk space used.
dfs.namenode.available-space-rack-fault-tolerant-block-placement-policy.balanced-space-tolerance:Only used when the dfs.block.replicator.classname is set to
    org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy.
    Special value between 0 and 20, inclusive. if the value is set beyond the scope,
    this value will be set as 5 by default, Increases tolerance of
    placing blocks on Datanodes with similar disk space used.
dfs.namenode.edits.dir.minimum:dfs.namenode.edits.dir includes both required directories
    (specified by dfs.namenode.edits.dir.required) and optional directories.

    The number of usable optional directories must be greater than or equal
    to this property.  If the number of usable optional directories falls
    below dfs.namenode.edits.dir.minimum, HDFS will issue an error.

    This property defaults to 1.
dfs.namenode.file.close.num-committed-allowed:Normally a file can only be closed with all its blocks are committed.
    When this value is set to a positive integer N, a file can be closed
    when N blocks are committed and the rest complete.
dfs.namenode.max-num-blocks-to-log:Puts a limit on the number of blocks printed to the log by the Namenode
    after a block report.
dfs.namenode.max.op.size:Maximum opcode size in bytes.
dfs.namenode.missing.checkpoint.periods.before.shutdown:The number of checkpoint period windows (as defined by the property
    dfs.namenode.checkpoint.period) allowed by the Namenode to perform
    saving the namespace before shutdown.
dfs.namenode.name.cache.threshold:Frequently accessed files that are accessed more times than this
    threshold are cached in the FSDirectory nameCache.
dfs.namenode.replication.max-streams:Hard limit for the number of replication streams other than those with highest-priority.
dfs.namenode.replication.max-streams-hard-limit:Hard limit for all replication streams.
dfs.namenode.reconstruction.pending.timeout-sec:Timeout in seconds for block reconstruction.  If this value is 0 or less,
    then it will default to 5 minutes.
dfs.namenode.stale.datanode.minimum.interval:Minimum number of missed heartbeats intervals for a datanode to
    be marked stale by the Namenode.  The actual interval is calculated as
    (dfs.namenode.stale.datanode.minimum.interval * dfs.heartbeat.interval)
    in seconds.  If this value is greater than the property
    dfs.namenode.stale.datanode.interval, then the calculated value above
    is used.
dfs.namenode.snapshotdiff.listing.limit:Limit the number of entries generated by getSnapshotDiffReportListing within
    one rpc call to the namenode.If less or equal to zero, at most
    DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT_DEFAULT (= 1000) will be sent
    across to the client within one rpc call.
dfs.namenode.snapshot.max.limit:Limits the maximum number of snapshots allowed per snapshottable
    directory.If the configuration is not set, the default limit
    for maximum no of snapshots allowed is 65536.
dfs.namenode.snapshot.skiplist.max.levels:Maximum no of the skip levels to be maintained in the skip list for
    storing directory snapshot diffs. By default, it is set to 0 and a linear
    list will be used to store the directory snapshot diffs.
dfs.namenode.snapshot.skiplist.interval:The interval after which the skip levels will be formed in the skip list
    for storing directory snapshot diffs. By default, value is set to 10.
dfs.storage.policy.satisfier.queue.limit:Storage policy satisfier queue size. This queue contains the currently
    scheduled file's inode ID for statisfy the policy.
    Default value is 1000.
dfs.storage.policy.satisfier.work.multiplier.per.iteration:*Note*: Advanced property. Change with caution.
    This determines the total amount of block transfers to begin in
    one iteration, for satisfy the policy. The actual number is obtained by
    multiplying this multiplier with the total number of live nodes in the
    cluster. The result number is the number of blocks to begin transfers
    immediately. This number can be any positive, non-zero integer.
dfs.storage.policy.satisfier.recheck.timeout.millis:Blocks storage movements monitor re-check interval in milliseconds.
    This check will verify whether any blocks storage movement results arrived from DN
    and also verify if any of file blocks movements not at all reported to DN
    since dfs.storage.policy.satisfier.self.retry.timeout.
    The default value is 1 * 60 * 1000 (1 mins)
dfs.storage.policy.satisfier.self.retry.timeout.millis:If any of file related block movements not at all reported by datanode,
    then after this timeout(in milliseconds), the item will be added back to movement needed list
    at namenode which will be retried for block movements.
    The default value is 5 * 60 * 1000 (5 mins)
dfs.storage.policy.satisfier.retry.max.attempts:Max retry to satisfy the block storage policy. After this retry block will be removed
    from the movement needed queue.
dfs.storage.policy.satisfier.datanode.cache.refresh.interval.ms:How often to refresh the datanode storages cache in milliseconds. This cache
    keeps live datanode storage reports fetched from namenode. After elapsed time,
    it will again fetch latest datanodes from namenode.
    By default, this parameter is set to 5 minutes.
dfs.storage.policy.satisfier.max.outstanding.paths:Defines the maximum number of paths to satisfy that can be queued up in the
    Satisfier call queue in a period of time. Default value is 10000.
dfs.qjournal.accept-recovery.timeout.ms:Quorum timeout in milliseconds during accept phase of
    recovery/synchronization for a specific segment.
dfs.qjournal.finalize-segment.timeout.ms:Quorum timeout in milliseconds during finalizing for a specific
    segment.
dfs.qjournal.get-journal-state.timeout.ms:Timeout in milliseconds when calling getJournalState().
    JournalNodes.
dfs.qjournal.new-epoch.timeout.ms:Timeout in milliseconds when getting an epoch number for write
    access to JournalNodes.
dfs.qjournal.prepare-recovery.timeout.ms:Quorum timeout in milliseconds during preparation phase of
    recovery/synchronization for a specific segment.
dfs.qjournal.queued-edits.limit.mb:Queue size in MB for quorum journal edits.
dfs.qjournal.select-input-streams.timeout.ms:Timeout in milliseconds for accepting streams from JournalManagers.
dfs.qjournal.start-segment.timeout.ms:Quorum timeout in milliseconds for starting a log segment.
dfs.qjournal.write-txns.timeout.ms:Write timeout in milliseconds when writing to a quorum of remote
    journals.
dfs.qjournal.http.open.timeout.ms:Timeout in milliseconds when open a new HTTP connection to remote
    journals.
dfs.qjournal.http.read.timeout.ms:Timeout in milliseconds when reading from a HTTP connection from remote
    journals.
dfs.qjournal.parallel-read.num-threads:Number of threads per JN to be used for tailing edits.
dfs.webhdfs.netty.high.watermark:High watermark configuration to Netty for Datanode WebHdfs.
dfs.webhdfs.netty.low.watermark:Low watermark configuration to Netty for Datanode WebHdfs.
dfs.disk.balancer.max.disk.throughputInMBperSec:Maximum disk bandwidth used by diskbalancer
      during read from a source disk. The unit is MB/sec.
dfs.disk.balancer.block.tolerance.percent:When a disk balancer copy operation is proceeding, the datanode is still
      active. So it might not be possible to move the exactly specified
      amount of data. So tolerance allows us to define a percentage which
      defines a good enough move.
dfs.disk.balancer.max.disk.errors:During a block move from a source to destination disk, we might
      encounter various errors. This defines how many errors we can tolerate
      before we declare a move between 2 disks (or a step) has failed.
dfs.disk.balancer.plan.threshold.percent:The percentage threshold value for volume Data Density in a plan.
      If the absolute value of volume Data Density which is out of
      threshold value in a node, it means that the volumes corresponding to
      the disks should do the balancing in the plan. The default value is 10.
dfs.provided.aliasmap.inmemory.batch-size:The batch size when iterating over the database backing the aliasmap
dfs.provided.aliasmap.load.retries:The number of retries on the Datanode to load the provided aliasmap;
      defaults to 0.
httpfs.buffer.size:The size buffer to be used when creating or opening httpfs filesystem IO stream.
dfs.namenode.block.deletion.increment:The number of block deletion increment.
      This setting will control the block increment deletion rate to
      ensure that other waiters on the lock can get in.
dfs.namenode.blockreport.queue.size:The queue size of BlockReportProcessingThread in BlockManager.
dfs.namenode.storage.dir.perm:Permissions for the directories on on the local filesystem where
      the DFS namenode stores the fsImage. The permissions can either be
      octal or symbolic.
dfs.namenode.blockreport.max.lock.hold.time:The BlockReportProcessingThread max write lock hold time in ms.
dfs.journalnode.edits.dir.perm:Permissions for the directories on on the local filesystem where
      the DFS journal node stores the edits. The permissions can either be
      octal or symbolic.
dfs.namenode.lease-hard-limit-sec:Determines the namenode automatic lease recovery interval in seconds.
mapreduce.task.io.sort.factor:The number of streams to merge at once while sorting
  files.  This determines the number of open file handles.
mapreduce.task.io.sort.mb:The total amount of buffer memory to use while sorting
  files, in megabytes.  By default, gives each merge stream 1MB, which
  should minimize seeks.
mapreduce.job.local-fs.single-disk-limit.check.interval-ms:Interval of disk limit check to run in ms.
mapreduce.job.maps:The default number of map tasks per job.
  Ignored when mapreduce.framework.name is "local".
mapreduce.job.reduces:The default number of reduce tasks per job. Typically set to 99%
  of the cluster's reduce capacity, so that if a node fails the reduces can
  still be executed in a single wave.
  Ignored when mapreduce.framework.name is "local".
mapreduce.job.running.map.limit:The maximum number of simultaneous map tasks per job.
  There is no limit if this value is 0 or negative.
mapreduce.job.running.reduce.limit:The maximum number of simultaneous reduce tasks per job.
  There is no limit if this value is 0 or negative.
mapreduce.job.reducer.preempt.delay.sec:The threshold (in seconds) after which an unsatisfied
      mapper request triggers reducer preemption when there is no anticipated
      headroom. If set to 0 or a negative value, the reducer is preempted as
      soon as lack of headroom is detected. Default is 0.
mapreduce.job.reducer.unconditional-preempt.delay.sec:The threshold (in seconds) after which an unsatisfied
      mapper request triggers a forced reducer preemption irrespective of the
      anticipated headroom. By default, it is set to 5 mins. Setting it to 0
      leads to immediate reducer preemption. Setting to -1 disables this
      preemption altogether.
mapreduce.job.max.split.locations:The max number of block locations to store for each split for
    locality calculation.
mapreduce.job.split.metainfo.maxsize:The maximum permissible size of the split metainfo file.
  The MapReduce ApplicationMaster won't attempt to read submitted split metainfo
  files bigger than this configured value.
  No limits if set to -1.
mapreduce.map.maxattempts:Expert: The maximum number of attempts per map task.
  In other words, framework will try to execute a map task these many number
  of times before giving up on it.
mapreduce.reduce.maxattempts:Expert: The maximum number of attempts per reduce task.
  In other words, framework will try to execute a reduce task these many number
  of times before giving up on it.
mapreduce.reduce.shuffle.fetch.retry.interval-ms:Time of interval that fetcher retry to fetch again when some
  non-fatal failure happens because of some events like NM restart.
mapreduce.reduce.shuffle.fetch.retry.timeout-ms:Timeout value for fetcher to retry to fetch again when some
  non-fatal failure happens because of some events like NM restart.
mapreduce.reduce.shuffle.retry-delay.max.ms:The maximum number of ms the reducer will delay before retrying
  to download map data.
mapreduce.reduce.shuffle.parallelcopies:The default number of parallel transfers run by reduce
  during the copy(shuffle) phase.
mapreduce.reduce.shuffle.connect.timeout:Expert: The maximum amount of time (in milli seconds) reduce
  task spends in trying to connect to a remote node for getting map output.
mapreduce.reduce.shuffle.read.timeout:Expert: The maximum amount of time (in milli seconds) reduce
  task waits for map output data to be available for reading after obtaining
  connection.
mapreduce.shuffle.listen.queue.size:The length of the shuffle server listen queue.
mapreduce.shuffle.connection-keep-alive.timeout:The number of seconds a shuffle client attempts to retain
   http connection. Refer "Keep-Alive: timeout=" header in
   Http specification
mapreduce.task.timeout:The number of milliseconds before a task will be
  terminated if it neither reads an input, writes an output, nor
  updates its status string.  A value of 0 disables the timeout.
mapreduce.task.stuck.timeout-ms:The max timeout before receiving remote task's first heartbeat.
    This parameter is in order to avoid waiting for the container
    to start indefinitely, which made task stuck in the NEW state.
    A value of 0 disables the timeout.
mapreduce.map.cpu.vcores:The number of virtual cores to request from the scheduler for
  each map task.
mapreduce.reduce.cpu.vcores:The number of virtual cores to request from the scheduler for
  each reduce task.
mapreduce.reduce.merge.inmem.threshold:The threshold, in terms of the number of files
  for the in-memory merge process. When we accumulate threshold number of files
  we initiate the in-memory merge and spill to disk. A value of 0 or less than
  0 indicates we want to DON'T have any threshold and instead depend only on
  the ramfs's memory consumption to trigger the merge.
mapreduce.shuffle.ssl.file.buffer.size:Buffer size for reading spills from file when using SSL.
mapreduce.shuffle.max.connections:Max allowed connections for the shuffle.  Set to 0 (zero)
               to indicate no limit on the number of connections.
mapreduce.shuffle.max.threads:Max allowed threads for serving shuffle connections. Set to zero
  to indicate the default of 2 times the number of available
  processors (as reported by Runtime.availableProcessors()). Netty is used to
  serve requests, so a thread is not needed for each connection.
mapreduce.shuffle.transfer.buffer.size:This property is used only if
  mapreduce.shuffle.transferTo.allowed is set to false. In that case,
  this property defines the size of the buffer used in the buffer copy code
  for the shuffle phase. The size of this buffer determines the size of the IO
  requests.
mapreduce.job.speculative.minimum-allowed-tasks:The minimum allowed tasks that
  can be speculatively re-executed at any time.
mapreduce.job.speculative.retry-after-no-speculate:The waiting time(ms) to do next round of speculation
  if there is no task speculated in this round.
mapreduce.job.speculative.retry-after-speculate:The waiting time(ms) to do next round of speculation
  if there are tasks speculated in this round.
mapreduce.job.ubertask.maxmaps:Threshold for number of maps, beyond which job is considered
  too big for the ubertasking optimization.  Users may override this value,
  but only downward.
mapreduce.job.ubertask.maxreduces:Threshold for number of reduces, beyond which job is considered
  too big for the ubertasking optimization.  CURRENTLY THE CODE CANNOT SUPPORT
  MORE THAN ONE REDUCE and will ignore larger values.  (Zero is a valid max,
  however.)  Users may override this value, but only downward.
mapreduce.input.fileinputformat.split.minsize:The minimum size chunk that map input should be split
  into.  Note that some file formats may have minimum split sizes that
  take priority over this setting.
mapreduce.input.fileinputformat.list-status.num-threads:The number of threads to use to list and fetch block locations
  for the specified input paths. Note: multiple threads should not be used
  if a custom non thread-safe path filter is used.
mapreduce.input.lineinputformat.linespermap:When using NLineInputFormat, the number of lines of input data
  to include in each split.
mapreduce.client.submit.file.replication:The replication level for submitted job files.  This
  should be around the square root of the number of nodes.
mapreduce.task.userlog.limit.kb:The maximum size of user-logs of each task in KB. 0 disables the cap.
yarn.app.mapreduce.am.container.log.limit.kb:The maximum size of the MRAppMaster attempt container logs in KB.
    0 disables the cap.
yarn.app.mapreduce.task.container.log.backups:Number of backup files for task logs when using
    ContainerRollingLogAppender (CRLA). See
    org.apache.log4j.RollingFileAppender.maxBackupIndex. By default,
    ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA
    is enabled for tasks when both mapreduce.task.userlog.limit.kb and
    yarn.app.mapreduce.task.container.log.backups are greater than zero.
yarn.app.mapreduce.am.container.log.backups:Number of backup files for the ApplicationMaster logs when using
    ContainerRollingLogAppender (CRLA). See
    org.apache.log4j.RollingFileAppender.maxBackupIndex. By default,
    ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA
    is enabled for the ApplicationMaster when both
    yarn.app.mapreduce.am.container.log.limit.kb and
    yarn.app.mapreduce.am.container.log.backups are greater than zero.
yarn.app.mapreduce.shuffle.log.limit.kb:Maximum size of the syslog.shuffle file in kilobytes
    (0 for no limit).
yarn.app.mapreduce.shuffle.log.backups:If yarn.app.mapreduce.shuffle.log.limit.kb and
    yarn.app.mapreduce.shuffle.log.backups are greater than zero
    then a ContainerRollngLogAppender is used instead of ContainerLogAppender
    for syslog.shuffle. See
    org.apache.log4j.RollingFileAppender.maxBackupIndex
mapreduce.job.maxtaskfailures.per.tracker:The number of task-failures on a node manager of a given job
               after which new tasks of that job aren't assigned to it. It
               MUST be less than mapreduce.map.maxattempts and
               mapreduce.reduce.maxattempts otherwise the failed task will
               never be tried on a different node.
mapreduce.client.completion.pollinterval:The interval (in milliseconds) between which the JobClient
    polls the MapReduce ApplicationMaster for updates about job status. You may want to
    set this to a lower value to make tests run faster on a single node system. Adjusting
    this value in production may lead to unwanted client-server traffic.
mapreduce.client.progressmonitor.pollinterval:The interval (in milliseconds) between which the JobClient
    reports status to the console and checks for job completion. You may want to set this
    to a lower value to make tests run faster on a single node system. Adjusting
    this value in production may lead to unwanted client-server traffic.
mapreduce.task.skip.start.attempts:The number of Task attempts AFTER which skip mode
    will be kicked off. When skip mode is kicked off, the
    tasks reports the range of records which it will process
    next, to the MR ApplicationMaster. So that on failures, the MR AM
    knows which ones are possibly the bad records. On further executions,
    those are skipped.
mapreduce.map.skip.maxrecords:The number of acceptable skip records surrounding the bad
    record PER bad record in mapper. The number includes the bad record as well.
    To turn the feature of detection/skipping of bad records off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever records(depends on application) get skipped are
    acceptable.
mapreduce.reduce.skip.maxgroups:The number of acceptable skip groups surrounding the bad
    group PER bad group in reducer. The number includes the bad group as well.
    To turn the feature of detection/skipping of bad groups off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever groups(depends on application) get skipped are
    acceptable.
mapreduce.ifile.readahead.bytes:Configuration key to set the IFile readahead length in bytes.
mapreduce.task.merge.progress.records:The number of records to process during merge before
   sending a progress notification to the MR ApplicationMaster.
mapreduce.task.combine.progress.records:The number of records to process during combine output collection
   before sending a progress notification.
mapreduce.shuffle.port:Default port that the ShuffleHandler will run on. ShuffleHandler
   is a service run at the NodeManager to facilitate transfers of intermediate
   Map outputs to requesting Reducers.
mapreduce.shuffle.pathcache.max-weight:The maximum total weight of entries the cache may contain.
mapreduce.shuffle.pathcache.expire-after-access-minutes:The length of time after an entry is last accessed that it
    should be automatically removed.
mapreduce.shuffle.pathcache.concurrency-level:Uses the concurrency level to create a fixed number of hashtable
    segments, each governed by its own write lock.
mapreduce.job.counters.max:The max number of user counters allowed per job.
mapreduce.am.max-attempts:The maximum number of application attempts. It is a
  application-specific setting. It should not be larger than the global number
  set by resourcemanager. Otherwise, it will be override. The default number is
  set to 2, to allow at least one retry for AM.
mapreduce.job.end-notification.retry.attempts:The number of times the submitter of the job wants to retry job
    end notification if it fails. This is capped by
    mapreduce.job.end-notification.max.attempts
mapreduce.job.end-notification.retry.interval:The number of milliseconds the submitter of the job wants to
    wait before job end notification is retried if it fails. This is capped by
    mapreduce.job.end-notification.max.retry.interval
mapreduce.job.end-notification.max.attempts:The maximum number of times a URL will be read for providing job
    end notification. Cluster administrators can set this to limit how long
    after end of a job, the Application Master waits before exiting. Must be
    marked as final to prevent users from overriding this.
mapreduce.job.end-notification.max.retry.interval:The maximum amount of time (in milliseconds) to wait before
     retrying job end notification. Cluster administrators can set this to
     limit how long the Application Master waits before exiting. Must be marked
     as final to prevent users from overriding this.
yarn.app.mapreduce.am.job.task.listener.thread-count:The number of threads used to handle RPC calls in the
    MR AppMaster from remote tasks
yarn.app.mapreduce.am.job.committer.cancel-timeout:The amount of time in milliseconds to wait for the output
    committer to cancel an operation if the job is killed
yarn.app.mapreduce.am.job.committer.commit-window:Defines a time window in milliseconds for output commit
  operations.  If contact with the RM has occurred within this window then
  commits are allowed, otherwise the AM will not allow output commits until
  contact with the RM has been re-established.
mapreduce.fileoutputcommitter.algorithm.version:The file output committer algorithm version
  valid algorithm version number: 1 or 2
  default to 2, which is the original algorithm

  In algorithm version 1,

  1. commitTask will rename directory
  $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/
  to
  $joboutput/_temporary/$appAttemptID/$taskID/

  2. recoverTask will also do a rename
  $joboutput/_temporary/$appAttemptID/$taskID/
  to
  $joboutput/_temporary/($appAttemptID + 1)/$taskID/

  3. commitJob will merge every task output file in
  $joboutput/_temporary/$appAttemptID/$taskID/
  to
  $joboutput/, then it will delete $joboutput/_temporary/
  and write $joboutput/_SUCCESS

  It has a performance regression, which is discussed in MAPREDUCE-4815.
  If a job generates many files to commit then the commitJob
  method call at the end of the job can take minutes.
  the commit is single-threaded and waits until all
  tasks have completed before commencing.

  algorithm version 2 will change the behavior of commitTask,
  recoverTask, and commitJob.

  1. commitTask will rename all files in
  $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/
  to $joboutput/

  2. recoverTask actually doesn't require to do anything, but for
  upgrade from version 1 to version 2 case, it will check if there
  are any files in
  $joboutput/_temporary/($appAttemptID - 1)/$taskID/
  and rename them to $joboutput/

  3. commitJob can simply delete $joboutput/_temporary and write
  $joboutput/_SUCCESS

  This algorithm will reduce the output commit time for
  large jobs by having the tasks commit directly to the final
  output directory as they were completing and commitJob had
  very little to do.
yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms:The interval in ms at which the MR AppMaster should send
    heartbeats to the ResourceManager
yarn.app.mapreduce.client-am.ipc.max-retries:The number of client retries to the AM - before reconnecting
    to the RM to fetch Application Status.
    In other words, it is the ipc.client.connect.max.retries to be used during
    reconnecting to the RM and fetching Application Status.
yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts:The number of client retries on socket timeouts to the AM - before
    reconnecting to the RM to fetch Application Status.
    In other words, it is the ipc.client.connect.max.retries.on.timeouts to be used during
    reconnecting to the RM and fetching Application Status.
yarn.app.mapreduce.client.max-retries:The number of client retries to the RM/HS before
    throwing exception. This is a layer above the ipc.
yarn.app.mapreduce.am.resource.mb:The amount of memory the MR AppMaster needs.
yarn.app.mapreduce.am.resource.cpu-vcores:The number of virtual CPU cores the MR AppMaster needs.
yarn.app.mapreduce.am.hard-kill-timeout-ms:Number of milliseconds to wait before the job client kills the application.
yarn.app.mapreduce.client.job.max-retries:The number of retries the client will make for getJob and
    dependent calls.
    This is needed for non-HDFS DFS where additional, high level
    retries are required to avoid spurious failures during the getJob call.
    30 is a good value for WASB
yarn.app.mapreduce.client.job.retry-interval:The delay between getJob retries in ms for retries configured
  with yarn.app.mapreduce.client.job.max-retries.
mapreduce.jobhistory.intermediate-user-done-dir.permissions:The permissions of the user directories in
  ${mapreduce.jobhistory.intermediate-done-dir}. The user and the group
  permission must be 7, this is enforced.
mapreduce.jobhistory.cleaner.interval-ms:How often the job history cleaner checks for files to delete,
  in milliseconds. Defaults to 86400000 (one day). Files are only deleted if
  they are older than mapreduce.jobhistory.max-age-ms.
mapreduce.jobhistory.max-age-ms:Job history files older than this many milliseconds will
  be deleted when the history cleaner runs. Defaults to 604800000 (1 week).
mapreduce.jobhistory.client.thread-count:The number of threads to handle client API requests
mapreduce.jobhistory.datestring.cache.size:Size of the date string cache. Effects the number of directories
  which will be scanned to find a job.
mapreduce.jobhistory.joblist.cache.size:Size of the job list cache
mapreduce.jobhistory.loadedjobs.cache.size:Size of the loaded job cache.  This property is ignored if
  the property mapreduce.jobhistory.loadedtasks.cache.size is set to a
  positive value.
mapreduce.jobhistory.move.interval-ms:Scan for history files to more from intermediate done dir to done
  dir at this frequency.
mapreduce.jobhistory.move.thread-count:The number of threads used to move files.
mapreduce.jobhistory.jobname.limit:Number of characters allowed for job name in Job History Server web page.
yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size:The initial size of thread pool to launch containers in the
    app master.
mapreduce.task.exit.timeout:The number of milliseconds before a task will be
  terminated if it stays in finishing state for too long.
  After a task attempt completes from TaskUmbilicalProtocol's point of view,
  it will be transitioned to finishing state. That will give a chance for the
  task to exit by itself.
mapreduce.task.exit.timeout.check-interval-ms:The interval in milliseconds between which the MR framework
  checks if task attempts stay in finishing state for too long.
mapreduce.job.encrypted-intermediate-data-key-size-bits:Mapreduce encrypt data key size default is 128
mapreduce.job.encrypted-intermediate-data.buffer.kb:Buffer size for intermediate encrypt data in kb
  default is 128
mapreduce.job.cache.limit.max-resources:The maximum number of resources a map reduce job is allowed to
    submit for localization via files, libjars, archives, and jobjar command
    line arguments and through the distributed cache. If set to 0 the limit is
    ignored.
mapreduce.job.cache.limit.max-resources-mb:The maximum size (in MB) a map reduce job is allowed to submit
    for localization via files, libjars, archives, and jobjar command line
    arguments and through the distributed cache. If set to 0 the limit is
    ignored.
mapreduce.job.cache.limit.max-single-resource-mb:The maximum size (in MB) of a single resource a map reduce job
    is allow to submit for localization via files, libjars, archives, and
    jobjar command line arguments and through the distributed cache. If set to
    0 the limit is ignored.
yarn.resourcemanager.client.thread-count:<missing>
yarn.resourcemanager.amlauncher.thread-count:<missing>
yarn.resourcemanager.nodemanager-connect-retries:<missing>
yarn.dispatcher.drain-events.timeout:<missing>
yarn.dispatcher.print-events-info.threshold:<missing>
yarn.dispatcher.cpu-monitor.samples-per-min:<missing>
yarn.am.liveness-monitor.expiry-interval-ms:<missing>
yarn.resourcemanager.scheduler.client.thread-count:<missing>
yarn.resourcemanager.placement-constraints.retry-attempts:<missing>
yarn.resourcemanager.placement-constraints.algorithm.pool-size:<missing>
yarn.resourcemanager.placement-constraints.scheduler.pool-size:<missing>
yarn.resourcemanager.admin.client.thread-count:<missing>
yarn.resourcemanager.connect.max-wait.ms:<missing>
yarn.resourcemanager.connect.retry-interval.ms:<missing>
yarn.resourcemanager.am.max-attempts:<missing>
yarn.resourcemanager.container.liveness-monitor.interval-ms:<missing>
yarn.nm.liveness-monitor.expiry-interval-ms:<missing>
yarn.resourcemanager.resource-tracker.client.thread-count:<missing>
yarn.scheduler.minimum-allocation-mb:<missing>
yarn.scheduler.maximum-allocation-mb:<missing>
yarn.scheduler.minimum-allocation-vcores:<missing>
yarn.scheduler.maximum-allocation-vcores:<missing>
yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms:<missing>
yarn.resourcemanager.fs.state-store.num-retries:<missing>
yarn.resourcemanager.fs.state-store.retry-interval-ms:<missing>
yarn.resourcemanager.leveldb-state-store.compaction-interval-secs:<missing>
yarn.resourcemanager.zk-appid-node.split-index:<missing>
yarn.resourcemanager.zk-delegation-token-node.split-index:<missing>
yarn.resourcemanager.zk-max-znode-size.bytes:<missing>
yarn.resourcemanager.epoch.range:<missing>
yarn.client.failover-retries:<missing>
yarn.client.failover-retries-on-socket-timeouts:<missing>
yarn.resourcemanager.max-completed-applications:<missing>
yarn.resourcemanager.delayed.delegation-token.removal-interval-ms:<missing>
yarn.resourcemanager.delegation-token.max-conf-size-bytes:<missing>
yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs:<missing>
yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs:<missing>
yarn.resourcemanager.nodemanagers.heartbeat-interval-ms:<missing>
yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms:<missing>
yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms:<missing>
yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size:<missing>
yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.batch-size:<missing>
yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.interval-seconds:<missing>
yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory:<missing>
yarn.resourcemanager.delegation-token-renewer.thread-count:<missing>
yarn.resourcemanager.delegation.key.update-interval:<missing>
yarn.resourcemanager.delegation.token.max-lifetime:<missing>
yarn.resourcemanager.delegation.token.renew-interval:<missing>
yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts:<missing>
yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size:<missing>
yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs:<missing>
yarn.resourcemanager.reservation-system.planfollower.time-step:<missing>
yarn.resourcemanager.rm.container-allocation.expiry-interval-ms:<missing>
yarn.nodemanager.container-manager.thread-count:<missing>
yarn.nodemanager.collector-service.thread-count:<missing>
yarn.nodemanager.delete.thread-count:<missing>
yarn.nodemanager.container-executor.exit-code-file.timeout-ms:<missing>
yarn.nodemanager.opportunistic-containers-max-queue-length:<missing>
yarn.nodemanager.delete.debug-delay-sec:<missing>
yarn.nodemanager.local-cache.max-files-per-directory:<missing>
yarn.nodemanager.localizer.cache.cleanup.interval-ms:<missing>
yarn.nodemanager.localizer.cache.target-size-mb:<missing>
yarn.nodemanager.localizer.client.thread-count:<missing>
yarn.nodemanager.localizer.fetch.thread-count:<missing>
yarn.nodemanager.default-container-executor.log-dirs.permissions:<missing>
yarn.log-aggregation.debug.filesize:<missing>
yarn.log-aggregation-status.time-out.ms:<missing>
yarn.nodemanager.log.retain-seconds:<missing>
yarn.nodemanager.resource.memory.cgroups.swappiness:<missing>
yarn.nodemanager.logaggregation.threadpool-size-max:<missing>
yarn.nodemanager.resource.percentage-physical-cpu-limit:<missing>
yarn.nodemanager.resource-monitor.interval-ms:<missing>
yarn.nodemanager.container-log-monitor.interval-ms:<missing>
yarn.nodemanager.container-log-monitor.dir-size-limit-bytes:<missing>
yarn.nodemanager.container-log-monitor.total-size-limit-bytes:<missing>
yarn.nodemanager.health-checker.timeout-ms:<missing>
yarn.nodemanager.health-checker.interval-ms:<missing>
yarn.nodemanager.disk-health-checker.interval-ms:<missing>
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb:<missing>
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb:<missing>
yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms:<missing>
yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold:<missing>
yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold:<missing>
yarn.nodemanager.runtime.linux.docker.stop.grace-period:<missing>
yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep:<missing>
yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs:<missing>
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs:<missing>
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache:<missing>
yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs:<missing>
yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size:<missing>
yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms:<missing>
yarn.nodemanager.aux-services.manifest.reload-ms:<missing>
yarn.nodemanager.sleep-delay-before-sigkill.ms:<missing>
yarn.nodemanager.process-kill-wait.ms:<missing>
yarn.nodemanager.container-diagnostics-maximum-size:<missing>
yarn.nodemanager.container-retry-minimum-interval-ms:<missing>
yarn.client.nodemanager-client-async.thread-pool-max-size:<missing>
yarn.client.nodemanager-connect.max-wait-ms:<missing>
yarn.client.nodemanager-connect.retry-interval-ms:<missing>
yarn.client.max-cached-nodemanagers-proxies:<missing>
yarn.nodemanager.recovery.compaction-interval-secs:<missing>
yarn.nodemanager.container-metrics.unregister-delay-ms:<missing>
yarn.nodemanager.log.deletion-threads-count:<missing>
yarn.resourcemanager.proxy.connection.timeout:<missing>
yarn.timeline-service.generic-application-history.max-applications:<missing>
yarn.timeline-service.ttl-ms:<missing>
yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms:<missing>
yarn.timeline-service.leveldb-timeline-store.read-cache-size:<missing>
yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size:<missing>
yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size:<missing>
yarn.timeline-service.handler-thread-count:<missing>
yarn.timeline-service.client.max-retries:<missing>
yarn.timeline-service.client.retry-interval-ms:<missing>
yarn.timeline-service.client.drain-entities.timeout.ms:<missing>
yarn.timeline-service.entity-group-fs-store.scan-interval-seconds:<missing>
yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds:<missing>
yarn.timeline-service.entity-group-fs-store.retain-seconds:<missing>
yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size:<missing>
yarn.timeline-service.entity-group-fs-store.app-cache-size:<missing>
yarn.timeline-service.client.fd-flush-interval-secs:<missing>
yarn.timeline-service.client.fd-clean-interval-secs:<missing>
yarn.timeline-service.client.fd-retain-secs:<missing>
yarn.timeline-service.client.internal-timers-ttl-secs:<missing>
yarn.timeline-service.writer.flush-interval-seconds:<missing>
yarn.timeline-service.writer.async.queue.capacity:<missing>
yarn.timeline-service.app-collector.linger-period.ms:<missing>
yarn.timeline-service.timeline-client.number-of-async-entities-to-merge:<missing>
yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds:<missing>
yarn.timeline-service.app-aggregation-interval-secs:<missing>
yarn.timeline-service.flowname.max-size:<missing>
yarn.sharedcache.nested-level:<missing>
yarn.sharedcache.store.in-memory.staleness-period-mins:<missing>
yarn.sharedcache.store.in-memory.initial-delay-mins:<missing>
yarn.sharedcache.store.in-memory.check-period-mins:<missing>
yarn.sharedcache.admin.thread-count:<missing>
yarn.sharedcache.cleaner.period-mins:<missing>
yarn.sharedcache.cleaner.initial-delay-mins:<missing>
yarn.sharedcache.cleaner.resource-sleep-ms:<missing>
yarn.sharedcache.uploader.server.thread-count:<missing>
yarn.sharedcache.client-server.thread-count:<missing>
yarn.sharedcache.nm.uploader.replication.factor:<missing>
yarn.sharedcache.nm.uploader.thread-count:<missing>
yarn.minicluster.yarn.nodemanager.resource.memory-mb:<missing>
yarn.nodemanager.node-labels.provider.fetch-interval-ms:<missing>
yarn.nodemanager.node-labels.resync-interval-ms:<missing>
yarn.nodemanager.node-labels.provider.fetch-timeout-ms:<missing>
yarn.resourcemanager.node-labels.provider.fetch-interval-ms:<missing>
yarn.nodemanager.node-attributes.provider.fetch-interval-ms:<missing>
yarn.nodemanager.node-attributes.provider.fetch-timeout-ms:<missing>
yarn.nodemanager.node-attributes.resync-interval-ms:<missing>
yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs:<missing>
yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs:<missing>
yarn.federation.cache-ttl.secs:<missing>
yarn.client.application-client-protocol.poll-interval-ms:<missing>
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min:<missing>
yarn.nodemanager.log-aggregation.num-log-files-per-app:<missing>
yarn.cluster.max-application-priority:<missing>
yarn.nodemanager.amrmproxy.client.thread-count:<missing>
yarn.resourcemanager.opportunistic-container-allocation.nodes-used:<missing>
yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms:<missing>
yarn.resourcemanager.nm-container-queuing.min-queue-length:<missing>
yarn.resourcemanager.nm-container-queuing.max-queue-length:<missing>
yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms:<missing>
yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms:<missing>
yarn.nodemanager.container.stderr.tail.bytes:<missing>
yarn.resourcemanager.node-removal-untracked.timeout-ms:<missing>
yarn.resourcemanager.application-timeouts.monitor.interval-ms:<missing>
yarn.app.attempt.diagnostics.limit.kc:<missing>
yarn.router.interceptor.user.threadpool-size:<missing>
yarn.router.pipeline.cache-max-size:<missing>
yarn.scheduler.configuration.leveldb-store.compaction-interval-secs:<missing>
yarn.scheduler.configuration.store.max-logs:<missing>
yarn.scheduler.configuration.max.version:<missing>
yarn.nodemanager.elastic-memory-control.timeout-sec:<missing>
yarn.resourcemanager.activities-manager.cleanup-interval-ms:<missing>
yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms:<missing>
yarn.resourcemanager.activities-manager.app-activities.ttl-ms:<missing>
yarn.resourcemanager.activities-manager.app-activities.max-queue-length:<missing>
yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms:<missing>
yarn.resourcemanager.application.max-tags:<missing>
yarn.resourcemanager.application.max-tag.length:<missing>
